{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow and Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow : \n",
    "* Tensorflow is developed by Google\n",
    "* Tensorflow is low level mathematics api , similar to numpy. But tensorflow is dedicated to deep learning.tensorflow is very efficent and faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "# checking the version of tensorflow \n",
    "import tensorflow as tf \n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method _OverrideBinaryOperatorHelper.<locals>.r_binary_op_wrapper of <tf.Tensor 'MatMul:0' shape=(1, 1) dtype=int32>>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "matrix1 = tf.constant([[4,4]])\n",
    "matrix2 = tf.constant([[5],[5]])\n",
    "ans = tf.matmul(matrix1, matrix2)\n",
    "ans.__rmatmul__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OVERLOADABLE_OPERATORS',\n",
       " '__abs__',\n",
       " '__add__',\n",
       " '__and__',\n",
       " '__array_priority__',\n",
       " '__bool__',\n",
       " '__class__',\n",
       " '__copy__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__div__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__floordiv__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__invert__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__matmul__',\n",
       " '__mod__',\n",
       " '__module__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__neg__',\n",
       " '__new__',\n",
       " '__nonzero__',\n",
       " '__or__',\n",
       " '__pow__',\n",
       " '__radd__',\n",
       " '__rand__',\n",
       " '__rdiv__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__rfloordiv__',\n",
       " '__rmatmul__',\n",
       " '__rmod__',\n",
       " '__rmul__',\n",
       " '__ror__',\n",
       " '__rpow__',\n",
       " '__rsub__',\n",
       " '__rtruediv__',\n",
       " '__rxor__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__sub__',\n",
       " '__subclasshook__',\n",
       " '__truediv__',\n",
       " '__weakref__',\n",
       " '__xor__',\n",
       " '_as_node_def_input',\n",
       " '_as_tf_output',\n",
       " '_c_api_shape',\n",
       " '_consumers',\n",
       " '_dtype',\n",
       " '_get_input_ops_without_shapes',\n",
       " '_id',\n",
       " '_op',\n",
       " '_override_operator',\n",
       " '_rank',\n",
       " '_shape',\n",
       " '_shape_as_list',\n",
       " '_shape_tuple',\n",
       " '_shape_val',\n",
       " '_tf_api_names',\n",
       " '_tf_api_names_v1',\n",
       " '_value_index',\n",
       " 'consumers',\n",
       " 'device',\n",
       " 'dtype',\n",
       " 'eval',\n",
       " 'get_shape',\n",
       " 'graph',\n",
       " 'name',\n",
       " 'op',\n",
       " 'set_shape',\n",
       " 'shape',\n",
       " 'value_index']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### if you want to build a big neural network then use google coLab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras\n",
    "* Keras is also created by google \n",
    "* Kears makes building a neural network simple\n",
    "* Keras runs on the Top of Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning is the the ability to train the dense Neaural Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculation of Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the neural network is the weighted sum of the all the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation functions\n",
    "\n",
    "* RELU : Rectified Linear Unit : $ \\phi(x) = \\max(0, x) $\n",
    "* Softmax : $ \\phi_i(z) = \\frac{e^{z_i}}{\\sum\\limits_{j \\in group}e^{z_j}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Basic models\n",
    "\n",
    "#### Basic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/670\n",
      " - 1s - loss: 1625276.8998\n",
      "Epoch 2/670\n",
      " - 0s - loss: 794473.9766\n",
      "Epoch 3/670\n",
      " - 0s - loss: 352823.6721\n",
      "Epoch 4/670\n",
      " - 0s - loss: 136572.0041\n",
      "Epoch 5/670\n",
      " - 0s - loss: 59121.9485\n",
      "Epoch 6/670\n",
      " - 0s - loss: 25559.3574\n",
      "Epoch 7/670\n",
      " - 0s - loss: 10552.9725\n",
      "Epoch 8/670\n",
      " - 0s - loss: 6439.5391\n",
      "Epoch 9/670\n",
      " - 0s - loss: 5286.1096\n",
      "Epoch 10/670\n",
      " - 0s - loss: 4637.6084\n",
      "Epoch 11/670\n",
      " - 0s - loss: 4157.0951\n",
      "Epoch 12/670\n",
      " - 0s - loss: 3758.0255\n",
      "Epoch 13/670\n",
      " - 0s - loss: 3401.0425\n",
      "Epoch 14/670\n",
      " - 0s - loss: 3078.1935\n",
      "Epoch 15/670\n",
      " - 0s - loss: 2781.3319\n",
      "Epoch 16/670\n",
      " - 0s - loss: 2515.3745\n",
      "Epoch 17/670\n",
      " - 0s - loss: 2276.2310\n",
      "Epoch 18/670\n",
      " - 0s - loss: 2051.6865\n",
      "Epoch 19/670\n",
      " - 0s - loss: 1854.7828\n",
      "Epoch 20/670\n",
      " - 0s - loss: 1671.3668\n",
      "Epoch 21/670\n",
      " - 0s - loss: 1506.9681\n",
      "Epoch 22/670\n",
      " - 0s - loss: 1359.9229\n",
      "Epoch 23/670\n",
      " - 0s - loss: 1229.3817\n",
      "Epoch 24/670\n",
      " - 0s - loss: 1108.1868\n",
      "Epoch 25/670\n",
      " - 0s - loss: 1003.4337\n",
      "Epoch 26/670\n",
      " - 0s - loss: 908.2205\n",
      "Epoch 27/670\n",
      " - 0s - loss: 822.2075\n",
      "Epoch 28/670\n",
      " - 0s - loss: 744.8900\n",
      "Epoch 29/670\n",
      " - 0s - loss: 677.5973\n",
      "Epoch 30/670\n",
      " - 0s - loss: 618.3749\n",
      "Epoch 31/670\n",
      " - 0s - loss: 565.6337\n",
      "Epoch 32/670\n",
      " - 0s - loss: 517.6536\n",
      "Epoch 33/670\n",
      " - 0s - loss: 475.9092\n",
      "Epoch 34/670\n",
      " - 0s - loss: 439.6876\n",
      "Epoch 35/670\n",
      " - 0s - loss: 406.8128\n",
      "Epoch 36/670\n",
      " - 0s - loss: 378.6381\n",
      "Epoch 37/670\n",
      " - 0s - loss: 352.8157\n",
      "Epoch 38/670\n",
      " - 0s - loss: 331.4364\n",
      "Epoch 39/670\n",
      " - 0s - loss: 311.4313\n",
      "Epoch 40/670\n",
      " - 0s - loss: 294.7774\n",
      "Epoch 41/670\n",
      " - 0s - loss: 279.9028\n",
      "Epoch 42/670\n",
      " - 0s - loss: 267.3764\n",
      "Epoch 43/670\n",
      " - 0s - loss: 255.4008\n",
      "Epoch 44/670\n",
      " - 0s - loss: 245.1913\n",
      "Epoch 45/670\n",
      " - 0s - loss: 236.0553\n",
      "Epoch 46/670\n",
      " - 0s - loss: 227.3430\n",
      "Epoch 47/670\n",
      " - 0s - loss: 219.3017\n",
      "Epoch 48/670\n",
      " - 0s - loss: 212.0770\n",
      "Epoch 49/670\n",
      " - 0s - loss: 205.2656\n",
      "Epoch 50/670\n",
      " - 0s - loss: 198.2305\n",
      "Epoch 51/670\n",
      " - 0s - loss: 191.5737\n",
      "Epoch 52/670\n",
      " - 0s - loss: 184.9608\n",
      "Epoch 53/670\n",
      " - 0s - loss: 179.0409\n",
      "Epoch 54/670\n",
      " - 0s - loss: 173.0467\n",
      "Epoch 55/670\n",
      " - 0s - loss: 167.5812\n",
      "Epoch 56/670\n",
      " - 0s - loss: 162.3239\n",
      "Epoch 57/670\n",
      " - 0s - loss: 157.5085\n",
      "Epoch 58/670\n",
      " - 0s - loss: 153.2035\n",
      "Epoch 59/670\n",
      " - 0s - loss: 149.3296\n",
      "Epoch 60/670\n",
      " - 0s - loss: 145.9800\n",
      "Epoch 61/670\n",
      " - 0s - loss: 142.9113\n",
      "Epoch 62/670\n",
      " - 0s - loss: 140.1498\n",
      "Epoch 63/670\n",
      " - 0s - loss: 137.8220\n",
      "Epoch 64/670\n",
      " - 0s - loss: 135.9343\n",
      "Epoch 65/670\n",
      " - 0s - loss: 134.3476\n",
      "Epoch 66/670\n",
      " - 0s - loss: 132.9006\n",
      "Epoch 67/670\n",
      " - 0s - loss: 131.6042\n",
      "Epoch 68/670\n",
      " - 0s - loss: 130.4447\n",
      "Epoch 69/670\n",
      " - 0s - loss: 129.4465\n",
      "Epoch 70/670\n",
      " - 0s - loss: 128.5334\n",
      "Epoch 71/670\n",
      " - 0s - loss: 127.7204\n",
      "Epoch 72/670\n",
      " - 0s - loss: 126.9714\n",
      "Epoch 73/670\n",
      " - 0s - loss: 126.2546\n",
      "Epoch 74/670\n",
      " - 0s - loss: 125.5036\n",
      "Epoch 75/670\n",
      " - 0s - loss: 124.7951\n",
      "Epoch 76/670\n",
      " - 0s - loss: 124.1390\n",
      "Epoch 77/670\n",
      " - 0s - loss: 123.4401\n",
      "Epoch 78/670\n",
      " - 0s - loss: 122.5797\n",
      "Epoch 79/670\n",
      " - 0s - loss: 121.3367\n",
      "Epoch 80/670\n",
      " - 0s - loss: 119.6919\n",
      "Epoch 81/670\n",
      " - 0s - loss: 116.8620\n",
      "Epoch 82/670\n",
      " - 0s - loss: 111.4320\n",
      "Epoch 83/670\n",
      " - 0s - loss: 105.3036\n",
      "Epoch 84/670\n",
      " - 0s - loss: 99.1671\n",
      "Epoch 85/670\n",
      " - 0s - loss: 94.8788\n",
      "Epoch 86/670\n",
      " - 0s - loss: 91.7637\n",
      "Epoch 87/670\n",
      " - 0s - loss: 89.6634\n",
      "Epoch 88/670\n",
      " - 0s - loss: 87.7793\n",
      "Epoch 89/670\n",
      " - 0s - loss: 86.3898\n",
      "Epoch 90/670\n",
      " - 0s - loss: 85.3060\n",
      "Epoch 91/670\n",
      " - 0s - loss: 84.5358\n",
      "Epoch 92/670\n",
      " - 0s - loss: 83.4012\n",
      "Epoch 93/670\n",
      " - 0s - loss: 82.7846\n",
      "Epoch 94/670\n",
      " - 0s - loss: 81.7852\n",
      "Epoch 95/670\n",
      " - 0s - loss: 81.2057\n",
      "Epoch 96/670\n",
      " - 0s - loss: 80.5933\n",
      "Epoch 97/670\n",
      " - 0s - loss: 79.9686\n",
      "Epoch 98/670\n",
      " - 0s - loss: 79.6212\n",
      "Epoch 99/670\n",
      " - 0s - loss: 79.2086\n",
      "Epoch 100/670\n",
      " - 0s - loss: 78.5368\n",
      "Epoch 101/670\n",
      " - 0s - loss: 78.0393\n",
      "Epoch 102/670\n",
      " - 0s - loss: 77.6614\n",
      "Epoch 103/670\n",
      " - 0s - loss: 77.2486\n",
      "Epoch 104/670\n",
      " - 0s - loss: 76.8796\n",
      "Epoch 105/670\n",
      " - 0s - loss: 76.5677\n",
      "Epoch 106/670\n",
      " - 0s - loss: 76.2443\n",
      "Epoch 107/670\n",
      " - 0s - loss: 75.9289\n",
      "Epoch 108/670\n",
      " - 0s - loss: 75.6413\n",
      "Epoch 109/670\n",
      " - 0s - loss: 75.4233\n",
      "Epoch 110/670\n",
      " - 0s - loss: 75.0963\n",
      "Epoch 111/670\n",
      " - 0s - loss: 75.3469\n",
      "Epoch 112/670\n",
      " - 0s - loss: 74.5586\n",
      "Epoch 113/670\n",
      " - 0s - loss: 74.3573\n",
      "Epoch 114/670\n",
      " - 0s - loss: 74.1619\n",
      "Epoch 115/670\n",
      " - 0s - loss: 73.9783\n",
      "Epoch 116/670\n",
      " - 0s - loss: 73.6477\n",
      "Epoch 117/670\n",
      " - 0s - loss: 73.5391\n",
      "Epoch 118/670\n",
      " - 0s - loss: 73.5327\n",
      "Epoch 119/670\n",
      " - 0s - loss: 72.9913\n",
      "Epoch 120/670\n",
      " - 0s - loss: 72.8352\n",
      "Epoch 121/670\n",
      " - 0s - loss: 72.6466\n",
      "Epoch 122/670\n",
      " - 0s - loss: 72.4049\n",
      "Epoch 123/670\n",
      " - 0s - loss: 72.3570\n",
      "Epoch 124/670\n",
      " - 0s - loss: 71.9894\n",
      "Epoch 125/670\n",
      " - 0s - loss: 71.9217\n",
      "Epoch 126/670\n",
      " - 0s - loss: 71.6385\n",
      "Epoch 127/670\n",
      " - 0s - loss: 71.4832\n",
      "Epoch 128/670\n",
      " - 0s - loss: 71.2043\n",
      "Epoch 129/670\n",
      " - 0s - loss: 71.1773\n",
      "Epoch 130/670\n",
      " - 0s - loss: 70.8982\n",
      "Epoch 131/670\n",
      " - 0s - loss: 70.8079\n",
      "Epoch 132/670\n",
      " - 0s - loss: 70.6515\n",
      "Epoch 133/670\n",
      " - 0s - loss: 70.4061\n",
      "Epoch 134/670\n",
      " - 0s - loss: 70.2427\n",
      "Epoch 135/670\n",
      " - 0s - loss: 69.9449\n",
      "Epoch 136/670\n",
      " - 0s - loss: 69.8326\n",
      "Epoch 137/670\n",
      " - 0s - loss: 69.5592\n",
      "Epoch 138/670\n",
      " - 0s - loss: 69.5142\n",
      "Epoch 139/670\n",
      " - 0s - loss: 69.4289\n",
      "Epoch 140/670\n",
      " - 0s - loss: 69.1099\n",
      "Epoch 141/670\n",
      " - 0s - loss: 68.8890\n",
      "Epoch 142/670\n",
      " - 0s - loss: 69.0224\n",
      "Epoch 143/670\n",
      " - 0s - loss: 68.6128\n",
      "Epoch 144/670\n",
      " - 0s - loss: 68.4103\n",
      "Epoch 145/670\n",
      " - 0s - loss: 68.3391\n",
      "Epoch 146/670\n",
      " - 0s - loss: 68.4705\n",
      "Epoch 147/670\n",
      " - 0s - loss: 67.8477\n",
      "Epoch 148/670\n",
      " - 0s - loss: 67.7551\n",
      "Epoch 149/670\n",
      " - 0s - loss: 67.5248\n",
      "Epoch 150/670\n",
      " - 0s - loss: 67.2489\n",
      "Epoch 151/670\n",
      " - 0s - loss: 67.1645\n",
      "Epoch 152/670\n",
      " - 0s - loss: 67.1921\n",
      "Epoch 153/670\n",
      " - 0s - loss: 66.8037\n",
      "Epoch 154/670\n",
      " - 0s - loss: 66.8538\n",
      "Epoch 155/670\n",
      " - 0s - loss: 66.4478\n",
      "Epoch 156/670\n",
      " - 0s - loss: 66.2681\n",
      "Epoch 157/670\n",
      " - 0s - loss: 66.1956\n",
      "Epoch 158/670\n",
      " - 0s - loss: 66.1153\n",
      "Epoch 159/670\n",
      " - 0s - loss: 65.7982\n",
      "Epoch 160/670\n",
      " - 0s - loss: 65.6480\n",
      "Epoch 161/670\n",
      " - 0s - loss: 65.6051\n",
      "Epoch 162/670\n",
      " - 0s - loss: 65.1602\n",
      "Epoch 163/670\n",
      " - 0s - loss: 65.1644\n",
      "Epoch 164/670\n",
      " - 0s - loss: 64.9112\n",
      "Epoch 165/670\n",
      " - 0s - loss: 64.7766\n",
      "Epoch 166/670\n",
      " - 0s - loss: 64.6136\n",
      "Epoch 167/670\n",
      " - 0s - loss: 64.3175\n",
      "Epoch 168/670\n",
      " - 0s - loss: 64.1790\n",
      "Epoch 169/670\n",
      " - 0s - loss: 63.9795\n",
      "Epoch 170/670\n",
      " - 0s - loss: 63.8967\n",
      "Epoch 171/670\n",
      " - 0s - loss: 63.8248\n",
      "Epoch 172/670\n",
      " - 0s - loss: 63.7027\n",
      "Epoch 173/670\n",
      " - 0s - loss: 63.4671\n",
      "Epoch 174/670\n",
      " - 0s - loss: 63.3300\n",
      "Epoch 175/670\n",
      " - 0s - loss: 63.0417\n",
      "Epoch 176/670\n",
      " - 0s - loss: 62.9844\n",
      "Epoch 177/670\n",
      " - 0s - loss: 62.6663\n",
      "Epoch 178/670\n",
      " - 0s - loss: 62.4898\n",
      "Epoch 179/670\n",
      " - 0s - loss: 62.3087\n",
      "Epoch 180/670\n",
      " - 0s - loss: 62.2992\n",
      "Epoch 181/670\n",
      " - 0s - loss: 62.0664\n",
      "Epoch 182/670\n",
      " - 0s - loss: 61.7643\n",
      "Epoch 183/670\n",
      " - 0s - loss: 61.6516\n",
      "Epoch 184/670\n",
      " - 0s - loss: 61.5019\n",
      "Epoch 185/670\n",
      " - 0s - loss: 61.4151\n",
      "Epoch 186/670\n",
      " - 0s - loss: 61.1593\n",
      "Epoch 187/670\n",
      " - 0s - loss: 60.9599\n",
      "Epoch 188/670\n",
      " - 0s - loss: 61.0699\n",
      "Epoch 189/670\n",
      " - 0s - loss: 60.9007\n",
      "Epoch 190/670\n",
      " - 0s - loss: 60.4619\n",
      "Epoch 191/670\n",
      " - 0s - loss: 60.3361\n",
      "Epoch 192/670\n",
      " - 0s - loss: 60.2243\n",
      "Epoch 193/670\n",
      " - 0s - loss: 60.3038\n",
      "Epoch 194/670\n",
      " - 0s - loss: 60.1229\n",
      "Epoch 195/670\n",
      " - 0s - loss: 59.6272\n",
      "Epoch 196/670\n",
      " - 0s - loss: 59.4893\n",
      "Epoch 197/670\n",
      " - 0s - loss: 59.3793\n",
      "Epoch 198/670\n",
      " - 0s - loss: 59.2819\n",
      "Epoch 199/670\n",
      " - 0s - loss: 58.9369\n",
      "Epoch 200/670\n",
      " - 0s - loss: 58.9874\n",
      "Epoch 201/670\n",
      " - 0s - loss: 58.9177\n",
      "Epoch 202/670\n",
      " - 0s - loss: 58.5751\n",
      "Epoch 203/670\n",
      " - 0s - loss: 58.2424\n",
      "Epoch 204/670\n",
      " - 0s - loss: 58.1692\n",
      "Epoch 205/670\n",
      " - 0s - loss: 58.1464\n",
      "Epoch 206/670\n",
      " - 0s - loss: 57.8305\n",
      "Epoch 207/670\n",
      " - 0s - loss: 57.6683\n",
      "Epoch 208/670\n",
      " - 0s - loss: 57.5079\n",
      "Epoch 209/670\n",
      " - 0s - loss: 57.3325\n",
      "Epoch 210/670\n",
      " - 0s - loss: 57.2665\n",
      "Epoch 211/670\n",
      " - 0s - loss: 57.3837\n",
      "Epoch 212/670\n",
      " - 0s - loss: 58.3896\n",
      "Epoch 213/670\n",
      " - 0s - loss: 56.6714\n",
      "Epoch 214/670\n",
      " - 0s - loss: 56.4869\n",
      "Epoch 215/670\n",
      " - 0s - loss: 56.6527\n",
      "Epoch 216/670\n",
      " - 0s - loss: 56.4034\n",
      "Epoch 217/670\n",
      " - 0s - loss: 56.0047\n",
      "Epoch 218/670\n",
      " - 0s - loss: 55.7734\n",
      "Epoch 219/670\n",
      " - 0s - loss: 55.6293\n",
      "Epoch 220/670\n",
      " - 0s - loss: 55.9447\n",
      "Epoch 221/670\n",
      " - 0s - loss: 55.5523\n",
      "Epoch 222/670\n",
      " - 0s - loss: 55.3221\n",
      "Epoch 223/670\n",
      " - 0s - loss: 55.0203\n",
      "Epoch 224/670\n",
      " - 0s - loss: 54.7479\n",
      "Epoch 225/670\n",
      " - 0s - loss: 54.6450\n",
      "Epoch 226/670\n",
      " - 0s - loss: 54.6000\n",
      "Epoch 227/670\n",
      " - 0s - loss: 54.2551\n",
      "Epoch 228/670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 54.0789\n",
      "Epoch 229/670\n",
      " - 0s - loss: 54.1886\n",
      "Epoch 230/670\n",
      " - 0s - loss: 53.8245\n",
      "Epoch 231/670\n",
      " - 0s - loss: 53.9949\n",
      "Epoch 232/670\n",
      " - 0s - loss: 53.3919\n",
      "Epoch 233/670\n",
      " - 0s - loss: 53.3235\n",
      "Epoch 234/670\n",
      " - 0s - loss: 53.1364\n",
      "Epoch 235/670\n",
      " - 0s - loss: 52.8955\n",
      "Epoch 236/670\n",
      " - 0s - loss: 52.7513\n",
      "Epoch 237/670\n",
      " - 0s - loss: 52.5845\n",
      "Epoch 238/670\n",
      " - 0s - loss: 52.3932\n",
      "Epoch 239/670\n",
      " - 0s - loss: 52.2236\n",
      "Epoch 240/670\n",
      " - 0s - loss: 52.1741\n",
      "Epoch 241/670\n",
      " - 0s - loss: 52.1191\n",
      "Epoch 242/670\n",
      " - 0s - loss: 51.7979\n",
      "Epoch 243/670\n",
      " - 0s - loss: 51.9495\n",
      "Epoch 244/670\n",
      " - 0s - loss: 51.4491\n",
      "Epoch 245/670\n",
      " - 0s - loss: 51.5180\n",
      "Epoch 246/670\n",
      " - 0s - loss: 51.1383\n",
      "Epoch 247/670\n",
      " - 0s - loss: 51.5793\n",
      "Epoch 248/670\n",
      " - 0s - loss: 50.9860\n",
      "Epoch 249/670\n",
      " - 0s - loss: 50.7171\n",
      "Epoch 250/670\n",
      " - 0s - loss: 50.5707\n",
      "Epoch 251/670\n",
      " - 0s - loss: 50.3542\n",
      "Epoch 252/670\n",
      " - 0s - loss: 50.1756\n",
      "Epoch 253/670\n",
      " - 0s - loss: 50.1940\n",
      "Epoch 254/670\n",
      " - 0s - loss: 49.9093\n",
      "Epoch 255/670\n",
      " - 0s - loss: 49.7181\n",
      "Epoch 256/670\n",
      " - 0s - loss: 49.5622\n",
      "Epoch 257/670\n",
      " - 0s - loss: 49.4564\n",
      "Epoch 258/670\n",
      " - 0s - loss: 49.1975\n",
      "Epoch 259/670\n",
      " - 0s - loss: 49.2679\n",
      "Epoch 260/670\n",
      " - 0s - loss: 49.0448\n",
      "Epoch 261/670\n",
      " - 0s - loss: 48.8036\n",
      "Epoch 262/670\n",
      " - 0s - loss: 48.5004\n",
      "Epoch 263/670\n",
      " - 0s - loss: 48.4086\n",
      "Epoch 264/670\n",
      " - 0s - loss: 48.5203\n",
      "Epoch 265/670\n",
      " - 0s - loss: 48.2386\n",
      "Epoch 266/670\n",
      " - 0s - loss: 48.1513\n",
      "Epoch 267/670\n",
      " - 0s - loss: 48.1317\n",
      "Epoch 268/670\n",
      " - 0s - loss: 47.7115\n",
      "Epoch 269/670\n",
      " - 0s - loss: 47.5687\n",
      "Epoch 270/670\n",
      " - 0s - loss: 47.3056\n",
      "Epoch 271/670\n",
      " - 0s - loss: 47.1351\n",
      "Epoch 272/670\n",
      " - 0s - loss: 47.1118\n",
      "Epoch 273/670\n",
      " - 0s - loss: 46.8862\n",
      "Epoch 274/670\n",
      " - 0s - loss: 46.7734\n",
      "Epoch 275/670\n",
      " - 0s - loss: 46.5662\n",
      "Epoch 276/670\n",
      " - 0s - loss: 46.4173\n",
      "Epoch 277/670\n",
      " - 0s - loss: 46.2045\n",
      "Epoch 278/670\n",
      " - 0s - loss: 46.0860\n",
      "Epoch 279/670\n",
      " - 0s - loss: 46.0085\n",
      "Epoch 280/670\n",
      " - 0s - loss: 45.9607\n",
      "Epoch 281/670\n",
      " - 0s - loss: 45.6398\n",
      "Epoch 282/670\n",
      " - 0s - loss: 45.5995\n",
      "Epoch 283/670\n",
      " - 0s - loss: 45.4032\n",
      "Epoch 284/670\n",
      " - 0s - loss: 45.0802\n",
      "Epoch 285/670\n",
      " - 0s - loss: 45.2997\n",
      "Epoch 286/670\n",
      " - 0s - loss: 44.8982\n",
      "Epoch 287/670\n",
      " - 0s - loss: 44.9178\n",
      "Epoch 288/670\n",
      " - 0s - loss: 44.5348\n",
      "Epoch 289/670\n",
      " - 0s - loss: 44.4675\n",
      "Epoch 290/670\n",
      " - 0s - loss: 44.4444\n",
      "Epoch 291/670\n",
      " - 0s - loss: 44.4709\n",
      "Epoch 292/670\n",
      " - 0s - loss: 43.9736\n",
      "Epoch 293/670\n",
      " - 0s - loss: 44.0075\n",
      "Epoch 294/670\n",
      " - 0s - loss: 44.2416\n",
      "Epoch 295/670\n",
      " - 0s - loss: 43.6712\n",
      "Epoch 296/670\n",
      " - 0s - loss: 43.3098\n",
      "Epoch 297/670\n",
      " - 0s - loss: 43.3930\n",
      "Epoch 298/670\n",
      " - 0s - loss: 43.1696\n",
      "Epoch 299/670\n",
      " - 0s - loss: 42.9838\n",
      "Epoch 300/670\n",
      " - 0s - loss: 42.7098\n",
      "Epoch 301/670\n",
      " - 0s - loss: 42.8253\n",
      "Epoch 302/670\n",
      " - 0s - loss: 43.0458\n",
      "Epoch 303/670\n",
      " - 0s - loss: 43.3394\n",
      "Epoch 304/670\n",
      " - 0s - loss: 42.6081\n",
      "Epoch 305/670\n",
      " - 0s - loss: 41.9391\n",
      "Epoch 306/670\n",
      " - 0s - loss: 42.0054\n",
      "Epoch 307/670\n",
      " - 0s - loss: 42.0001\n",
      "Epoch 308/670\n",
      " - 0s - loss: 41.6015\n",
      "Epoch 309/670\n",
      " - 0s - loss: 41.5751\n",
      "Epoch 310/670\n",
      " - 0s - loss: 41.5635\n",
      "Epoch 311/670\n",
      " - 0s - loss: 41.2771\n",
      "Epoch 312/670\n",
      " - 0s - loss: 41.0154\n",
      "Epoch 313/670\n",
      " - 0s - loss: 40.8822\n",
      "Epoch 314/670\n",
      " - 0s - loss: 40.7807\n",
      "Epoch 315/670\n",
      " - 0s - loss: 40.6021\n",
      "Epoch 316/670\n",
      " - 0s - loss: 40.6741\n",
      "Epoch 317/670\n",
      " - 0s - loss: 40.5997\n",
      "Epoch 318/670\n",
      " - 0s - loss: 40.5266\n",
      "Epoch 319/670\n",
      " - 0s - loss: 40.1157\n",
      "Epoch 320/670\n",
      " - 0s - loss: 40.0682\n",
      "Epoch 321/670\n",
      " - 0s - loss: 39.7741\n",
      "Epoch 322/670\n",
      " - 0s - loss: 39.7705\n",
      "Epoch 323/670\n",
      " - 0s - loss: 39.5402\n",
      "Epoch 324/670\n",
      " - 0s - loss: 39.3549\n",
      "Epoch 325/670\n",
      " - 0s - loss: 39.2311\n",
      "Epoch 326/670\n",
      " - 0s - loss: 39.0286\n",
      "Epoch 327/670\n",
      " - 0s - loss: 39.0085\n",
      "Epoch 328/670\n",
      " - 0s - loss: 39.4166\n",
      "Epoch 329/670\n",
      " - 0s - loss: 38.7909\n",
      "Epoch 330/670\n",
      " - 0s - loss: 38.5689\n",
      "Epoch 331/670\n",
      " - 0s - loss: 38.4727\n",
      "Epoch 332/670\n",
      " - 0s - loss: 38.3736\n",
      "Epoch 333/670\n",
      " - 0s - loss: 38.2639\n",
      "Epoch 334/670\n",
      " - 0s - loss: 38.3562\n",
      "Epoch 335/670\n",
      " - 0s - loss: 38.9438\n",
      "Epoch 336/670\n",
      " - 0s - loss: 38.5038\n",
      "Epoch 337/670\n",
      " - 0s - loss: 37.6138\n",
      "Epoch 338/670\n",
      " - 0s - loss: 37.4792\n",
      "Epoch 339/670\n",
      " - 0s - loss: 37.4581\n",
      "Epoch 340/670\n",
      " - 0s - loss: 37.5617\n",
      "Epoch 341/670\n",
      " - 0s - loss: 37.3347\n",
      "Epoch 342/670\n",
      " - 0s - loss: 37.0321\n",
      "Epoch 343/670\n",
      " - 0s - loss: 36.8168\n",
      "Epoch 344/670\n",
      " - 0s - loss: 37.3494\n",
      "Epoch 345/670\n",
      " - 0s - loss: 37.4046\n",
      "Epoch 346/670\n",
      " - 0s - loss: 36.6298\n",
      "Epoch 347/670\n",
      " - 0s - loss: 36.9016\n",
      "Epoch 348/670\n",
      " - 0s - loss: 36.7750\n",
      "Epoch 349/670\n",
      " - 0s - loss: 36.5856\n",
      "Epoch 350/670\n",
      " - 0s - loss: 36.0367\n",
      "Epoch 351/670\n",
      " - 0s - loss: 35.7527\n",
      "Epoch 352/670\n",
      " - 0s - loss: 35.7448\n",
      "Epoch 353/670\n",
      " - 0s - loss: 35.4748\n",
      "Epoch 354/670\n",
      " - 0s - loss: 35.4146\n",
      "Epoch 355/670\n",
      " - 0s - loss: 35.1559\n",
      "Epoch 356/670\n",
      " - 0s - loss: 35.0680\n",
      "Epoch 357/670\n",
      " - 0s - loss: 34.9260\n",
      "Epoch 358/670\n",
      " - 0s - loss: 35.5854\n",
      "Epoch 359/670\n",
      " - 0s - loss: 35.2853\n",
      "Epoch 360/670\n",
      " - 0s - loss: 35.1534\n",
      "Epoch 361/670\n",
      " - 0s - loss: 34.9580\n",
      "Epoch 362/670\n",
      " - 0s - loss: 34.2145\n",
      "Epoch 363/670\n",
      " - 0s - loss: 34.1708\n",
      "Epoch 364/670\n",
      " - 0s - loss: 34.3123\n",
      "Epoch 365/670\n",
      " - 0s - loss: 34.0240\n",
      "Epoch 366/670\n",
      " - 0s - loss: 33.7510\n",
      "Epoch 367/670\n",
      " - 0s - loss: 33.7190\n",
      "Epoch 368/670\n",
      " - 0s - loss: 33.4626\n",
      "Epoch 369/670\n",
      " - 0s - loss: 33.4218\n",
      "Epoch 370/670\n",
      " - 0s - loss: 33.5107\n",
      "Epoch 371/670\n",
      " - 0s - loss: 33.2123\n",
      "Epoch 372/670\n",
      " - 0s - loss: 33.1458\n",
      "Epoch 373/670\n",
      " - 0s - loss: 33.8839\n",
      "Epoch 374/670\n",
      " - 0s - loss: 32.9848\n",
      "Epoch 375/670\n",
      " - 0s - loss: 32.5659\n",
      "Epoch 376/670\n",
      " - 0s - loss: 32.5247\n",
      "Epoch 377/670\n",
      " - 0s - loss: 32.5769\n",
      "Epoch 378/670\n",
      " - 0s - loss: 32.1768\n",
      "Epoch 379/670\n",
      " - 0s - loss: 32.0805\n",
      "Epoch 380/670\n",
      " - 0s - loss: 32.0717\n",
      "Epoch 381/670\n",
      " - 0s - loss: 31.8583\n",
      "Epoch 382/670\n",
      " - 0s - loss: 31.8747\n",
      "Epoch 383/670\n",
      " - 0s - loss: 31.4830\n",
      "Epoch 384/670\n",
      " - 0s - loss: 31.5169\n",
      "Epoch 385/670\n",
      " - 0s - loss: 31.4320\n",
      "Epoch 386/670\n",
      " - 0s - loss: 31.2011\n",
      "Epoch 387/670\n",
      " - 0s - loss: 31.3451\n",
      "Epoch 388/670\n",
      " - 0s - loss: 31.0446\n",
      "Epoch 389/670\n",
      " - 0s - loss: 30.9069\n",
      "Epoch 390/670\n",
      " - 0s - loss: 31.6307\n",
      "Epoch 391/670\n",
      " - 0s - loss: 30.9710\n",
      "Epoch 392/670\n",
      " - 0s - loss: 31.1444\n",
      "Epoch 393/670\n",
      " - 0s - loss: 30.4404\n",
      "Epoch 394/670\n",
      " - 0s - loss: 30.1000\n",
      "Epoch 395/670\n",
      " - 0s - loss: 30.4533\n",
      "Epoch 396/670\n",
      " - 0s - loss: 31.3531\n",
      "Epoch 397/670\n",
      " - 0s - loss: 29.8754\n",
      "Epoch 398/670\n",
      " - 0s - loss: 29.7377\n",
      "Epoch 399/670\n",
      " - 0s - loss: 29.5543\n",
      "Epoch 400/670\n",
      " - 0s - loss: 29.3806\n",
      "Epoch 401/670\n",
      " - 0s - loss: 29.2464\n",
      "Epoch 402/670\n",
      " - 0s - loss: 29.3498\n",
      "Epoch 403/670\n",
      " - 0s - loss: 28.9244\n",
      "Epoch 404/670\n",
      " - 0s - loss: 29.0097\n",
      "Epoch 405/670\n",
      " - 0s - loss: 29.0011\n",
      "Epoch 406/670\n",
      " - 0s - loss: 28.6237\n",
      "Epoch 407/670\n",
      " - 0s - loss: 28.5011\n",
      "Epoch 408/670\n",
      " - 0s - loss: 28.4498\n",
      "Epoch 409/670\n",
      " - 0s - loss: 28.1616\n",
      "Epoch 410/670\n",
      " - 0s - loss: 28.0981\n",
      "Epoch 411/670\n",
      " - 0s - loss: 28.0093\n",
      "Epoch 412/670\n",
      " - 0s - loss: 27.9613\n",
      "Epoch 413/670\n",
      " - 0s - loss: 27.6059\n",
      "Epoch 414/670\n",
      " - 0s - loss: 27.4882\n",
      "Epoch 415/670\n",
      " - 0s - loss: 27.3688\n",
      "Epoch 416/670\n",
      " - 0s - loss: 27.1511\n",
      "Epoch 417/670\n",
      " - 0s - loss: 27.1261\n",
      "Epoch 418/670\n",
      " - 0s - loss: 26.8582\n",
      "Epoch 419/670\n",
      " - 0s - loss: 27.0989\n",
      "Epoch 420/670\n",
      " - 0s - loss: 26.5737\n",
      "Epoch 421/670\n",
      " - 0s - loss: 26.5122\n",
      "Epoch 422/670\n",
      " - 0s - loss: 26.5608\n",
      "Epoch 423/670\n",
      " - 0s - loss: 26.4774\n",
      "Epoch 424/670\n",
      " - 0s - loss: 25.9710\n",
      "Epoch 425/670\n",
      " - 0s - loss: 26.3277\n",
      "Epoch 426/670\n",
      " - 0s - loss: 26.3698\n",
      "Epoch 427/670\n",
      " - 0s - loss: 26.0854\n",
      "Epoch 428/670\n",
      " - 0s - loss: 26.2988\n",
      "Epoch 429/670\n",
      " - 0s - loss: 25.4447\n",
      "Epoch 430/670\n",
      " - 0s - loss: 24.9411\n",
      "Epoch 431/670\n",
      " - 0s - loss: 24.9537\n",
      "Epoch 432/670\n",
      " - 0s - loss: 24.9911\n",
      "Epoch 433/670\n",
      " - 0s - loss: 24.2940\n",
      "Epoch 434/670\n",
      " - 0s - loss: 24.3507\n",
      "Epoch 435/670\n",
      " - 0s - loss: 24.2410\n",
      "Epoch 436/670\n",
      " - 0s - loss: 23.7816\n",
      "Epoch 437/670\n",
      " - 0s - loss: 23.5428\n",
      "Epoch 438/670\n",
      " - 0s - loss: 23.2592\n",
      "Epoch 439/670\n",
      " - 0s - loss: 22.7398\n",
      "Epoch 440/670\n",
      " - 0s - loss: 22.2060\n",
      "Epoch 441/670\n",
      " - 0s - loss: 22.2530\n",
      "Epoch 442/670\n",
      " - 0s - loss: 21.5783\n",
      "Epoch 443/670\n",
      " - 0s - loss: 21.3962\n",
      "Epoch 444/670\n",
      " - 0s - loss: 21.0084\n",
      "Epoch 445/670\n",
      " - 0s - loss: 21.4524\n",
      "Epoch 446/670\n",
      " - 0s - loss: 20.8300\n",
      "Epoch 447/670\n",
      " - 0s - loss: 20.5625\n",
      "Epoch 448/670\n",
      " - 0s - loss: 20.3807\n",
      "Epoch 449/670\n",
      " - 0s - loss: 20.1418\n",
      "Epoch 450/670\n",
      " - 0s - loss: 20.0357\n",
      "Epoch 451/670\n",
      " - 0s - loss: 19.8419\n",
      "Epoch 452/670\n",
      " - 0s - loss: 19.6494\n",
      "Epoch 453/670\n",
      " - 0s - loss: 19.9019\n",
      "Epoch 454/670\n",
      " - 0s - loss: 19.5800\n",
      "Epoch 455/670\n",
      " - 0s - loss: 19.3948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 456/670\n",
      " - 0s - loss: 19.1078\n",
      "Epoch 457/670\n",
      " - 0s - loss: 18.9399\n",
      "Epoch 458/670\n",
      " - 0s - loss: 18.8807\n",
      "Epoch 459/670\n",
      " - 0s - loss: 18.7756\n",
      "Epoch 460/670\n",
      " - 0s - loss: 18.6485\n",
      "Epoch 461/670\n",
      " - 0s - loss: 18.8042\n",
      "Epoch 462/670\n",
      " - 0s - loss: 18.1986\n",
      "Epoch 463/670\n",
      " - 0s - loss: 18.3194\n",
      "Epoch 464/670\n",
      " - 0s - loss: 18.1873\n",
      "Epoch 465/670\n",
      " - 0s - loss: 18.1286\n",
      "Epoch 466/670\n",
      " - 0s - loss: 17.9636\n",
      "Epoch 467/670\n",
      " - 0s - loss: 17.6099\n",
      "Epoch 468/670\n",
      " - 0s - loss: 17.6212\n",
      "Epoch 469/670\n",
      " - 0s - loss: 18.1659\n",
      "Epoch 470/670\n",
      " - 0s - loss: 18.8369\n",
      "Epoch 471/670\n",
      " - 0s - loss: 18.2421\n",
      "Epoch 472/670\n",
      " - 0s - loss: 17.9996\n",
      "Epoch 473/670\n",
      " - 0s - loss: 18.0190\n",
      "Epoch 474/670\n",
      " - 0s - loss: 17.2542\n",
      "Epoch 475/670\n",
      " - 0s - loss: 17.3042\n",
      "Epoch 476/670\n",
      " - 0s - loss: 17.1826\n",
      "Epoch 477/670\n",
      " - 0s - loss: 16.9223\n",
      "Epoch 478/670\n",
      " - 0s - loss: 16.5230\n",
      "Epoch 479/670\n",
      " - 0s - loss: 16.7150\n",
      "Epoch 480/670\n",
      " - 0s - loss: 16.6387\n",
      "Epoch 481/670\n",
      " - 0s - loss: 16.3199\n",
      "Epoch 482/670\n",
      " - 0s - loss: 16.2990\n",
      "Epoch 483/670\n",
      " - 0s - loss: 16.8419\n",
      "Epoch 484/670\n",
      " - 0s - loss: 16.1704\n",
      "Epoch 485/670\n",
      " - 0s - loss: 16.5239\n",
      "Epoch 486/670\n",
      " - 0s - loss: 17.0734\n",
      "Epoch 487/670\n",
      " - 0s - loss: 16.5605\n",
      "Epoch 488/670\n",
      " - 0s - loss: 16.2954\n",
      "Epoch 489/670\n",
      " - 0s - loss: 15.9209\n",
      "Epoch 490/670\n",
      " - 0s - loss: 15.7236\n",
      "Epoch 491/670\n",
      " - 0s - loss: 15.6215\n",
      "Epoch 492/670\n",
      " - 0s - loss: 15.5231\n",
      "Epoch 493/670\n",
      " - 0s - loss: 15.7907\n",
      "Epoch 494/670\n",
      " - 0s - loss: 15.7127\n",
      "Epoch 495/670\n",
      " - 0s - loss: 15.5224\n",
      "Epoch 496/670\n",
      " - 0s - loss: 15.2551\n",
      "Epoch 497/670\n",
      " - 0s - loss: 15.0240\n",
      "Epoch 498/670\n",
      " - 0s - loss: 14.9266\n",
      "Epoch 499/670\n",
      " - 0s - loss: 15.2876\n",
      "Epoch 500/670\n",
      " - 0s - loss: 15.5748\n",
      "Epoch 501/670\n",
      " - 0s - loss: 15.6240\n",
      "Epoch 502/670\n",
      " - 0s - loss: 15.7630\n",
      "Epoch 503/670\n",
      " - 0s - loss: 14.6868\n",
      "Epoch 504/670\n",
      " - 0s - loss: 14.5983\n",
      "Epoch 505/670\n",
      " - 0s - loss: 14.4738\n",
      "Epoch 506/670\n",
      " - 0s - loss: 14.3909\n",
      "Epoch 507/670\n",
      " - 0s - loss: 14.3757\n",
      "Epoch 508/670\n",
      " - 0s - loss: 14.2899\n",
      "Epoch 509/670\n",
      " - 0s - loss: 14.3246\n",
      "Epoch 510/670\n",
      " - 0s - loss: 14.0908\n",
      "Epoch 511/670\n",
      " - 0s - loss: 13.9671\n",
      "Epoch 512/670\n",
      " - 0s - loss: 14.7711\n",
      "Epoch 513/670\n",
      " - 0s - loss: 14.7160\n",
      "Epoch 514/670\n",
      " - 0s - loss: 14.0260\n",
      "Epoch 515/670\n",
      " - 0s - loss: 13.8922\n",
      "Epoch 516/670\n",
      " - 0s - loss: 13.8477\n",
      "Epoch 517/670\n",
      " - 0s - loss: 13.9740\n",
      "Epoch 518/670\n",
      " - 0s - loss: 14.1977\n",
      "Epoch 519/670\n",
      " - 0s - loss: 14.3807\n",
      "Epoch 520/670\n",
      " - 0s - loss: 13.7308\n",
      "Epoch 521/670\n",
      " - 0s - loss: 13.8684\n",
      "Epoch 522/670\n",
      " - 0s - loss: 14.0676\n",
      "Epoch 523/670\n",
      " - 0s - loss: 13.7502\n",
      "Epoch 524/670\n",
      " - 0s - loss: 13.2980\n",
      "Epoch 525/670\n",
      " - 0s - loss: 13.2341\n",
      "Epoch 526/670\n",
      " - 0s - loss: 13.2710\n",
      "Epoch 527/670\n",
      " - 0s - loss: 13.1146\n",
      "Epoch 528/670\n",
      " - 0s - loss: 13.2051\n",
      "Epoch 529/670\n",
      " - 0s - loss: 13.0788\n",
      "Epoch 530/670\n",
      " - 0s - loss: 12.9560\n",
      "Epoch 531/670\n",
      " - 0s - loss: 13.5783\n",
      "Epoch 532/670\n",
      " - 0s - loss: 13.3985\n",
      "Epoch 533/670\n",
      " - 0s - loss: 12.8829\n",
      "Epoch 534/670\n",
      " - 0s - loss: 13.8356\n",
      "Epoch 535/670\n",
      " - 0s - loss: 12.5924\n",
      "Epoch 536/670\n",
      " - 0s - loss: 12.8863\n",
      "Epoch 537/670\n",
      " - 0s - loss: 12.7297\n",
      "Epoch 538/670\n",
      " - 0s - loss: 12.3843\n",
      "Epoch 539/670\n",
      " - 0s - loss: 12.9998\n",
      "Epoch 540/670\n",
      " - 0s - loss: 12.5248\n",
      "Epoch 541/670\n",
      " - 0s - loss: 13.2211\n",
      "Epoch 542/670\n",
      " - 0s - loss: 12.4163\n",
      "Epoch 543/670\n",
      " - 0s - loss: 12.4410\n",
      "Epoch 544/670\n",
      " - 0s - loss: 12.0751\n",
      "Epoch 545/670\n",
      " - 0s - loss: 11.8768\n",
      "Epoch 546/670\n",
      " - 0s - loss: 12.8857\n",
      "Epoch 547/670\n",
      " - 0s - loss: 12.2649\n",
      "Epoch 548/670\n",
      " - 0s - loss: 11.9372\n",
      "Epoch 549/670\n",
      " - 0s - loss: 12.0393\n",
      "Epoch 550/670\n",
      " - 0s - loss: 12.3141\n",
      "Epoch 551/670\n",
      " - 0s - loss: 12.3387\n",
      "Epoch 552/670\n",
      " - 0s - loss: 12.8486\n",
      "Epoch 553/670\n",
      " - 0s - loss: 13.1352\n",
      "Epoch 554/670\n",
      " - 0s - loss: 11.8609\n",
      "Epoch 555/670\n",
      " - 0s - loss: 13.2903\n",
      "Epoch 556/670\n",
      " - 0s - loss: 11.7217\n",
      "Epoch 557/670\n",
      " - 0s - loss: 11.9368\n",
      "Epoch 558/670\n",
      " - 0s - loss: 12.5805\n",
      "Epoch 559/670\n",
      " - 0s - loss: 12.2253\n",
      "Epoch 560/670\n",
      " - 0s - loss: 11.4840\n",
      "Epoch 561/670\n",
      " - 0s - loss: 11.5655\n",
      "Epoch 562/670\n",
      " - 0s - loss: 11.5766\n",
      "Epoch 563/670\n",
      " - 0s - loss: 12.0820\n",
      "Epoch 564/670\n",
      " - 0s - loss: 11.3804\n",
      "Epoch 565/670\n",
      " - 0s - loss: 11.6975\n",
      "Epoch 566/670\n",
      " - 0s - loss: 11.6483\n",
      "Epoch 567/670\n",
      " - 0s - loss: 11.2608\n",
      "Epoch 568/670\n",
      " - 0s - loss: 11.6004\n",
      "Epoch 569/670\n",
      " - 0s - loss: 13.1199\n",
      "Epoch 570/670\n",
      " - 0s - loss: 11.6143\n",
      "Epoch 571/670\n",
      " - 0s - loss: 11.1726\n",
      "Epoch 572/670\n",
      " - 0s - loss: 11.3239\n",
      "Epoch 573/670\n",
      " - 0s - loss: 11.0272\n",
      "Epoch 574/670\n",
      " - 0s - loss: 11.1299\n",
      "Epoch 575/670\n",
      " - 0s - loss: 11.0938\n",
      "Epoch 576/670\n",
      " - 0s - loss: 11.6967\n",
      "Epoch 577/670\n",
      " - 0s - loss: 11.4227\n",
      "Epoch 578/670\n",
      " - 0s - loss: 11.3437\n",
      "Epoch 579/670\n",
      " - 0s - loss: 11.4123\n",
      "Epoch 580/670\n",
      " - 0s - loss: 11.0249\n",
      "Epoch 581/670\n",
      " - 0s - loss: 11.3905\n",
      "Epoch 582/670\n",
      " - 0s - loss: 10.9359\n",
      "Epoch 583/670\n",
      " - 0s - loss: 10.7214\n",
      "Epoch 584/670\n",
      " - 0s - loss: 10.8772\n",
      "Epoch 585/670\n",
      " - 0s - loss: 10.8012\n",
      "Epoch 586/670\n",
      " - 0s - loss: 10.6408\n",
      "Epoch 587/670\n",
      " - 0s - loss: 10.8158\n",
      "Epoch 588/670\n",
      " - 0s - loss: 11.3762\n",
      "Epoch 589/670\n",
      " - 0s - loss: 10.7858\n",
      "Epoch 590/670\n",
      " - 0s - loss: 11.5600\n",
      "Epoch 591/670\n",
      " - 0s - loss: 12.7945\n",
      "Epoch 592/670\n",
      " - 0s - loss: 11.5474\n",
      "Epoch 593/670\n",
      " - 0s - loss: 10.8163\n",
      "Epoch 594/670\n",
      " - 0s - loss: 10.6784\n",
      "Epoch 595/670\n",
      " - 0s - loss: 10.5390\n",
      "Epoch 596/670\n",
      " - 0s - loss: 10.3165\n",
      "Epoch 597/670\n",
      " - 0s - loss: 10.4179\n",
      "Epoch 598/670\n",
      " - 0s - loss: 10.3968\n",
      "Epoch 599/670\n",
      " - 0s - loss: 10.7893\n",
      "Epoch 600/670\n",
      " - 0s - loss: 10.5002\n",
      "Epoch 601/670\n",
      " - 0s - loss: 10.4510\n",
      "Epoch 602/670\n",
      " - 0s - loss: 10.8406\n",
      "Epoch 603/670\n",
      " - 0s - loss: 11.5824\n",
      "Epoch 604/670\n",
      " - 0s - loss: 11.3962\n",
      "Epoch 605/670\n",
      " - 0s - loss: 10.9793\n",
      "Epoch 606/670\n",
      " - 0s - loss: 11.8872\n",
      "Epoch 607/670\n",
      " - 0s - loss: 10.8661\n",
      "Epoch 608/670\n",
      " - 0s - loss: 11.4920\n",
      "Epoch 609/670\n",
      " - 0s - loss: 10.5383\n",
      "Epoch 610/670\n",
      " - 0s - loss: 10.3542\n",
      "Epoch 611/670\n",
      " - 0s - loss: 10.8253\n",
      "Epoch 612/670\n",
      " - 0s - loss: 10.7227\n",
      "Epoch 613/670\n",
      " - 0s - loss: 10.5955\n",
      "Epoch 614/670\n",
      " - 0s - loss: 10.2020\n",
      "Epoch 615/670\n",
      " - 0s - loss: 12.3397\n",
      "Epoch 616/670\n",
      " - 0s - loss: 10.6494\n",
      "Epoch 617/670\n",
      " - 0s - loss: 10.8537\n",
      "Epoch 618/670\n",
      " - 0s - loss: 11.2062\n",
      "Epoch 619/670\n",
      " - 0s - loss: 11.5085\n",
      "Epoch 620/670\n",
      " - 0s - loss: 10.1756\n",
      "Epoch 621/670\n",
      " - 0s - loss: 10.6510\n",
      "Epoch 622/670\n",
      " - 0s - loss: 10.1442\n",
      "Epoch 623/670\n",
      " - 0s - loss: 10.3073\n",
      "Epoch 624/670\n",
      " - 0s - loss: 10.6958\n",
      "Epoch 625/670\n",
      " - 0s - loss: 10.2427\n",
      "Epoch 626/670\n",
      " - 0s - loss: 10.4909\n",
      "Epoch 627/670\n",
      " - 0s - loss: 10.6736\n",
      "Epoch 628/670\n",
      " - 0s - loss: 9.9855\n",
      "Epoch 629/670\n",
      " - 0s - loss: 10.2217\n",
      "Epoch 630/670\n",
      " - 0s - loss: 11.0429\n",
      "Epoch 631/670\n",
      " - 0s - loss: 10.6313\n",
      "Epoch 632/670\n",
      " - 0s - loss: 12.5764\n",
      "Epoch 633/670\n",
      " - 0s - loss: 11.6443\n",
      "Epoch 634/670\n",
      " - 0s - loss: 10.5675\n",
      "Epoch 635/670\n",
      " - 0s - loss: 10.1700\n",
      "Epoch 636/670\n",
      " - 0s - loss: 10.2215\n",
      "Epoch 637/670\n",
      " - 0s - loss: 10.8674\n",
      "Epoch 638/670\n",
      " - 0s - loss: 10.5938\n",
      "Epoch 639/670\n",
      " - 0s - loss: 10.1047\n",
      "Epoch 640/670\n",
      " - 0s - loss: 9.9069\n",
      "Epoch 641/670\n",
      " - 0s - loss: 10.7747\n",
      "Epoch 642/670\n",
      " - 0s - loss: 9.9219\n",
      "Epoch 643/670\n",
      " - 0s - loss: 10.1557\n",
      "Epoch 644/670\n",
      " - 0s - loss: 10.3421\n",
      "Epoch 645/670\n",
      " - 0s - loss: 9.9211\n",
      "Epoch 646/670\n",
      " - 0s - loss: 10.3611\n",
      "Epoch 647/670\n",
      " - 0s - loss: 10.4634\n",
      "Epoch 648/670\n",
      " - 0s - loss: 10.1173\n",
      "Epoch 649/670\n",
      " - 0s - loss: 10.5391\n",
      "Epoch 650/670\n",
      " - 0s - loss: 9.9117\n",
      "Epoch 651/670\n",
      " - 0s - loss: 10.0512\n",
      "Epoch 652/670\n",
      " - 0s - loss: 9.7914\n",
      "Epoch 653/670\n",
      " - 0s - loss: 11.7778\n",
      "Epoch 654/670\n",
      " - 0s - loss: 10.4883\n",
      "Epoch 655/670\n",
      " - 0s - loss: 11.1533\n",
      "Epoch 656/670\n",
      " - 0s - loss: 9.8397\n",
      "Epoch 657/670\n",
      " - 0s - loss: 10.1232\n",
      "Epoch 658/670\n",
      " - 0s - loss: 10.4773\n",
      "Epoch 659/670\n",
      " - 0s - loss: 10.8901\n",
      "Epoch 660/670\n",
      " - 0s - loss: 10.0418\n",
      "Epoch 661/670\n",
      " - 0s - loss: 10.4784\n",
      "Epoch 662/670\n",
      " - 0s - loss: 10.1415\n",
      "Epoch 663/670\n",
      " - 0s - loss: 10.0950\n",
      "Epoch 664/670\n",
      " - 0s - loss: 10.6170\n",
      "Epoch 665/670\n",
      " - 0s - loss: 10.3561\n",
      "Epoch 666/670\n",
      " - 0s - loss: 9.8803\n",
      "Epoch 667/670\n",
      " - 0s - loss: 9.9354\n",
      "Epoch 668/670\n",
      " - 0s - loss: 9.9631\n",
      "Epoch 669/670\n",
      " - 0s - loss: 9.9265\n",
      "Epoch 670/670\n",
      " - 0s - loss: 10.0089\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x11077cdf0f0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "import pandas as pd\n",
    "import io\n",
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "path = './data/'\n",
    "\n",
    "read_csv = os.path.join(path, \"auto-mpg.csv\")\n",
    "df = pd.read_csv(read_csv , na_values = ['NA' , '?'])\n",
    "\n",
    "cars = df['name']\n",
    "df['horsepower'] = df['horsepower'].fillna(df['horsepower'].median())\n",
    "\n",
    "x = df[['cylinders', 'displacement', 'horsepower', 'weight',\n",
    "       'acceleration', 'year', 'origin']].values\n",
    "y = df['mpg'].values\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=x.shape[1], activation='relu')) # Hidden 1\n",
    "model.add(Dense(10, activation='relu')) # Hidden 2\n",
    "model.add(Dense(1)) # Output\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(x,y,verbose=2,epochs=670)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12.776391 ]\n",
      " [13.52345  ]\n",
      " [13.248921 ]\n",
      " [13.340874 ]\n",
      " [12.989603 ]\n",
      " [13.936922 ]\n",
      " [14.404147 ]\n",
      " [14.28983  ]\n",
      " [14.570764 ]\n",
      " [13.880001 ]\n",
      " [13.528241 ]\n",
      " [13.244113 ]\n",
      " [12.86364  ]\n",
      " [14.959366 ]\n",
      " [23.955307 ]\n",
      " [19.55828  ]\n",
      " [19.891499 ]\n",
      " [22.11045  ]\n",
      " [25.977646 ]\n",
      " [31.251354 ]\n",
      " [21.954073 ]\n",
      " [22.868525 ]\n",
      " [23.695953 ]\n",
      " [22.61016  ]\n",
      " [21.121143 ]\n",
      " [14.710549 ]\n",
      " [14.567464 ]\n",
      " [14.722473 ]\n",
      " [14.478941 ]\n",
      " [26.549856 ]\n",
      " [24.299328 ]\n",
      " [25.370516 ]\n",
      " [26.36525  ]\n",
      " [20.90827  ]\n",
      " [15.135772 ]\n",
      " [16.372458 ]\n",
      " [17.265095 ]\n",
      " [16.592022 ]\n",
      " [13.542693 ]\n",
      " [13.582512 ]\n",
      " [13.317879 ]\n",
      " [13.309892 ]\n",
      " [13.617089 ]\n",
      " [13.394448 ]\n",
      " [13.414196 ]\n",
      " [18.11654  ]\n",
      " [25.087942 ]\n",
      " [16.60618  ]\n",
      " [18.250826 ]\n",
      " [24.41314  ]\n",
      " [25.671928 ]\n",
      " [28.321198 ]\n",
      " [26.894196 ]\n",
      " [31.500757 ]\n",
      " [32.23894  ]\n",
      " [30.668156 ]\n",
      " [28.723467 ]\n",
      " [25.925255 ]\n",
      " [26.632387 ]\n",
      " [29.50451  ]\n",
      " [24.744846 ]\n",
      " [25.537277 ]\n",
      " [13.66495  ]\n",
      " [13.77441  ]\n",
      " [13.467548 ]\n",
      " [13.438423 ]\n",
      " [13.525903 ]\n",
      " [14.458302 ]\n",
      " [13.414195 ]\n",
      " [13.561592 ]\n",
      " [14.193751 ]\n",
      " [24.676577 ]\n",
      " [13.517788 ]\n",
      " [12.997681 ]\n",
      " [13.332976 ]\n",
      " [13.512229 ]\n",
      " [19.038273 ]\n",
      " [25.113762 ]\n",
      " [21.318768 ]\n",
      " [27.812328 ]\n",
      " [24.14823  ]\n",
      " [26.30218  ]\n",
      " [23.893036 ]\n",
      " [25.871761 ]\n",
      " [27.825045 ]\n",
      " [14.180408 ]\n",
      " [13.667046 ]\n",
      " [13.408462 ]\n",
      " [13.383217 ]\n",
      " [13.656877 ]\n",
      " [14.26655  ]\n",
      " [13.222253 ]\n",
      " [13.645727 ]\n",
      " [13.634782 ]\n",
      " [14.729686 ]\n",
      " [14.89722  ]\n",
      " [14.124691 ]\n",
      " [18.90804  ]\n",
      " [18.497044 ]\n",
      " [20.432674 ]\n",
      " [20.759382 ]\n",
      " [20.860958 ]\n",
      " [32.22344  ]\n",
      " [13.178973 ]\n",
      " [13.577692 ]\n",
      " [13.860288 ]\n",
      " [14.170882 ]\n",
      " [21.36581  ]\n",
      " [27.64944  ]\n",
      " [26.404274 ]\n",
      " [26.010403 ]\n",
      " [27.198761 ]\n",
      " [26.013515 ]\n",
      " [22.7277   ]\n",
      " [26.019312 ]\n",
      " [13.381179 ]\n",
      " [15.282158 ]\n",
      " [32.173756 ]\n",
      " [27.773857 ]\n",
      " [23.300789 ]\n",
      " [20.337652 ]\n",
      " [13.683015 ]\n",
      " [21.657051 ]\n",
      " [20.813347 ]\n",
      " [14.329617 ]\n",
      " [20.06508  ]\n",
      " [21.985893 ]\n",
      " [21.335434 ]\n",
      " [18.394695 ]\n",
      " [31.81204  ]\n",
      " [25.336277 ]\n",
      " [33.22169  ]\n",
      " [25.147816 ]\n",
      " [15.051517 ]\n",
      " [15.874251 ]\n",
      " [15.783952 ]\n",
      " [13.548232 ]\n",
      " [13.5608835]\n",
      " [13.65636  ]\n",
      " [13.515391 ]\n",
      " [13.861188 ]\n",
      " [27.578323 ]\n",
      " [30.089064 ]\n",
      " [26.774635 ]\n",
      " [34.323822 ]\n",
      " [31.7784   ]\n",
      " [27.437893 ]\n",
      " [28.593906 ]\n",
      " [27.326797 ]\n",
      " [25.284609 ]\n",
      " [26.31121  ]\n",
      " [29.930483 ]\n",
      " [19.43361  ]\n",
      " [17.516653 ]\n",
      " [20.810383 ]\n",
      " [22.510674 ]\n",
      " [13.953751 ]\n",
      " [13.615258 ]\n",
      " [13.841309 ]\n",
      " [13.602333 ]\n",
      " [14.964143 ]\n",
      " [14.823066 ]\n",
      " [15.949037 ]\n",
      " [16.235836 ]\n",
      " [20.052464 ]\n",
      " [18.587538 ]\n",
      " [17.724438 ]\n",
      " [29.636189 ]\n",
      " [24.531464 ]\n",
      " [21.80999  ]\n",
      " [25.530186 ]\n",
      " [24.024693 ]\n",
      " [28.770878 ]\n",
      " [25.90878  ]\n",
      " [20.798498 ]\n",
      " [30.380009 ]\n",
      " [20.392414 ]\n",
      " [23.617554 ]\n",
      " [22.546236 ]\n",
      " [21.469498 ]\n",
      " [22.312021 ]\n",
      " [34.054256 ]\n",
      " [26.515099 ]\n",
      " [29.014153 ]\n",
      " [24.58823  ]\n",
      " [28.17916  ]\n",
      " [28.578632 ]\n",
      " [13.745714 ]\n",
      " [13.988245 ]\n",
      " [13.335517 ]\n",
      " [13.94001  ]\n",
      " [19.808027 ]\n",
      " [18.526987 ]\n",
      " [22.962994 ]\n",
      " [22.054478 ]\n",
      " [32.39467  ]\n",
      " [31.005663 ]\n",
      " [31.000023 ]\n",
      " [34.602543 ]\n",
      " [17.216877 ]\n",
      " [19.970894 ]\n",
      " [16.451904 ]\n",
      " [21.131247 ]\n",
      " [31.340763 ]\n",
      " [32.03541  ]\n",
      " [30.424131 ]\n",
      " [25.48     ]\n",
      " [20.602451 ]\n",
      " [14.071932 ]\n",
      " [21.937077 ]\n",
      " [22.888248 ]\n",
      " [15.068163 ]\n",
      " [14.606477 ]\n",
      " [13.756715 ]\n",
      " [13.696683 ]\n",
      " [14.170188 ]\n",
      " [32.731537 ]\n",
      " [28.81658  ]\n",
      " [34.10004  ]\n",
      " [27.028786 ]\n",
      " [32.897907 ]\n",
      " [14.090314 ]\n",
      " [14.732238 ]\n",
      " [14.048808 ]\n",
      " [13.708913 ]\n",
      " [18.01092  ]\n",
      " [19.037994 ]\n",
      " [17.946873 ]\n",
      " [19.284054 ]\n",
      " [14.611877 ]\n",
      " [14.502874 ]\n",
      " [14.910813 ]\n",
      " [14.060565 ]\n",
      " [31.195885 ]\n",
      " [24.445011 ]\n",
      " [30.599743 ]\n",
      " [24.173027 ]\n",
      " [31.036572 ]\n",
      " [29.44394  ]\n",
      " [32.73337  ]\n",
      " [29.222172 ]\n",
      " [24.668724 ]\n",
      " [24.110092 ]\n",
      " [23.909657 ]\n",
      " [34.79163  ]\n",
      " [32.701145 ]\n",
      " [34.790775 ]\n",
      " [32.96087  ]\n",
      " [35.068275 ]\n",
      " [19.690098 ]\n",
      " [14.921833 ]\n",
      " [16.044758 ]\n",
      " [19.333069 ]\n",
      " [22.371544 ]\n",
      " [23.800663 ]\n",
      " [24.970078 ]\n",
      " [19.902264 ]\n",
      " [22.164265 ]\n",
      " [19.685633 ]\n",
      " [23.226688 ]\n",
      " [18.25822  ]\n",
      " [18.563627 ]\n",
      " [16.899492 ]\n",
      " [15.175371 ]\n",
      " [18.404774 ]\n",
      " [14.0732565]\n",
      " [30.420618 ]\n",
      " [27.032764 ]\n",
      " [28.916824 ]\n",
      " [29.011639 ]\n",
      " [27.514093 ]\n",
      " [24.194164 ]\n",
      " [24.707796 ]\n",
      " [28.175728 ]\n",
      " [24.200304 ]\n",
      " [20.268934 ]\n",
      " [23.62241  ]\n",
      " [18.306023 ]\n",
      " [31.850977 ]\n",
      " [32.1738   ]\n",
      " [20.601704 ]\n",
      " [24.758163 ]\n",
      " [24.71888  ]\n",
      " [22.562126 ]\n",
      " [20.282215 ]\n",
      " [15.741507 ]\n",
      " [16.17055  ]\n",
      " [14.118373 ]\n",
      " [15.545308 ]\n",
      " [14.521227 ]\n",
      " [14.22549  ]\n",
      " [17.512075 ]\n",
      " [14.377928 ]\n",
      " [32.696613 ]\n",
      " [33.72723  ]\n",
      " [31.604649 ]\n",
      " [26.188965 ]\n",
      " [22.266716 ]\n",
      " [16.27696  ]\n",
      " [26.02757  ]\n",
      " [22.598663 ]\n",
      " [29.786213 ]\n",
      " [30.567808 ]\n",
      " [34.339405 ]\n",
      " [31.448492 ]\n",
      " [26.000397 ]\n",
      " [24.220848 ]\n",
      " [23.814137 ]\n",
      " [26.188227 ]\n",
      " [31.547579 ]\n",
      " [35.51386  ]\n",
      " [31.474285 ]\n",
      " [34.255432 ]\n",
      " [26.631908 ]\n",
      " [25.632378 ]\n",
      " [25.049932 ]\n",
      " [22.34775  ]\n",
      " [31.359827 ]\n",
      " [27.64067  ]\n",
      " [30.18136  ]\n",
      " [29.413563 ]\n",
      " [31.654104 ]\n",
      " [33.930016 ]\n",
      " [24.376253 ]\n",
      " [34.23549  ]\n",
      " [35.232533 ]\n",
      " [33.831936 ]\n",
      " [27.418608 ]\n",
      " [25.687195 ]\n",
      " [34.813854 ]\n",
      " [33.420055 ]\n",
      " [33.630024 ]\n",
      " [34.697643 ]\n",
      " [23.027676 ]\n",
      " [28.166943 ]\n",
      " [28.396797 ]\n",
      " [24.145882 ]\n",
      " [32.06364  ]\n",
      " [28.691662 ]\n",
      " [27.848171 ]\n",
      " [27.047966 ]\n",
      " [24.98643  ]\n",
      " [28.811874 ]\n",
      " [37.298386 ]\n",
      " [34.387867 ]\n",
      " [36.964626 ]\n",
      " [34.755512 ]\n",
      " [35.8697   ]\n",
      " [34.996887 ]\n",
      " [34.83977  ]\n",
      " [31.626238 ]\n",
      " [33.064518 ]\n",
      " [31.622341 ]\n",
      " [31.804207 ]\n",
      " [30.064117 ]\n",
      " [32.443577 ]\n",
      " [31.969757 ]\n",
      " [28.118372 ]\n",
      " [30.30333  ]\n",
      " [25.303486 ]\n",
      " [25.99619  ]\n",
      " [24.881306 ]\n",
      " [24.604366 ]\n",
      " [20.852037 ]\n",
      " [20.267723 ]\n",
      " [24.941595 ]\n",
      " [22.074835 ]\n",
      " [28.988235 ]\n",
      " [28.486528 ]\n",
      " [30.183853 ]\n",
      " [28.574184 ]\n",
      " [29.072624 ]\n",
      " [27.706282 ]\n",
      " [26.178543 ]\n",
      " [25.848312 ]\n",
      " [34.216755 ]\n",
      " [35.636745 ]\n",
      " [35.90669  ]\n",
      " [32.82691  ]\n",
      " [33.01096  ]\n",
      " [32.731922 ]\n",
      " [33.077213 ]\n",
      " [33.642128 ]\n",
      " [35.380833 ]\n",
      " [35.547993 ]\n",
      " [35.442043 ]\n",
      " [24.852623 ]\n",
      " [26.305248 ]\n",
      " [27.90702  ]\n",
      " [25.408806 ]\n",
      " [28.452873 ]\n",
      " [29.520668 ]\n",
      " [25.923904 ]\n",
      " [26.895826 ]\n",
      " [36.53589  ]\n",
      " [29.749844 ]\n",
      " [29.15586  ]\n",
      " [28.455841 ]]\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(x)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.107906901336322\n"
     ]
    }
   ],
   "source": [
    "score = np.sqrt(metrics.mean_squared_error(pred,y))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['sepal_l', 'sepal_w', 'petal_l', 'petal_w'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-67c63e83a31d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mread_csv\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mna_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'NA'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'?'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sepal_l'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sepal_w'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'petal_l'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'petal_w'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mdummies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'species'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mspecies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdummies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\bhavesh\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2979\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2980\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2981\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2982\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2983\u001b[0m         \u001b[1;31m# take() does not accept boolean indexers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\bhavesh\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[1;34m(self, obj, axis, is_setter, raise_missing)\u001b[0m\n\u001b[0;32m   1269\u001b[0m                 \u001b[1;31m# When setting, missing keys are not allowed, even with .loc:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1270\u001b[0m                 \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"raise_missing\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mis_setter\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1271\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1272\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1273\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\bhavesh\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1076\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1077\u001b[0m         self._validate_read_indexer(\n\u001b[1;32m-> 1078\u001b[1;33m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1079\u001b[0m         )\n\u001b[0;32m   1080\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\bhavesh\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[1;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1161\u001b[0m                 raise KeyError(\n\u001b[0;32m   1162\u001b[0m                     \"None of [{key}] are in the [{axis}]\".format(\n\u001b[1;32m-> 1163\u001b[1;33m                         \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1164\u001b[0m                     )\n\u001b[0;32m   1165\u001b[0m                 )\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['sepal_l', 'sepal_w', 'petal_l', 'petal_w'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "\n",
    "path = '../data/'\n",
    "\n",
    "read_csv = os.path.join(path, \"iris.csv\")\n",
    "df = pd.read_csv(read_csv , na_values = ['NA' , '?'])\n",
    "\n",
    "x = df[['sepal_l', 'sepal_w', 'petal_l', 'petal_w']].values\n",
    "dummies = pd.get_dummies(df['species'])\n",
    "species = dummies.columns\n",
    "y = dummies.values\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=x.shape[1], activation='relu')) \n",
    "model.add(Dense(25, activation='relu')) \n",
    "model.add(Dense(y.shape[1],activation='softmax')) \n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(x,y,verbose=2,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.9843055e-01 1.5641378e-03 5.2591490e-06]\n",
      " [9.9666256e-01 3.3242397e-03 1.3152626e-05]\n",
      " [9.9741495e-01 2.5721681e-03 1.2831969e-05]\n",
      " [9.9557531e-01 4.4021029e-03 2.2525666e-05]\n",
      " [9.9844164e-01 1.5524846e-03 5.7851939e-06]\n",
      " [9.9841189e-01 1.5840896e-03 3.9992724e-06]\n",
      " [9.9709928e-01 2.8848320e-03 1.5964331e-05]\n",
      " [9.9779379e-01 2.1984940e-03 7.7795785e-06]\n",
      " [9.9451858e-01 5.4470152e-03 3.4395376e-05]\n",
      " [9.9676090e-01 3.2271650e-03 1.1993140e-05]\n",
      " [9.9885225e-01 1.1448752e-03 2.8310430e-06]\n",
      " [9.9679381e-01 3.1928867e-03 1.3291380e-05]\n",
      " [9.9671602e-01 3.2702538e-03 1.3731657e-05]\n",
      " [9.9688798e-01 3.0888007e-03 2.3204289e-05]\n",
      " [9.9961531e-01 3.8394376e-04 7.4404699e-07]\n",
      " [9.9948728e-01 5.1167619e-04 1.0680973e-06]\n",
      " [9.9919230e-01 8.0538960e-04 2.3191292e-06]\n",
      " [9.9831784e-01 1.6763875e-03 5.8120750e-06]\n",
      " [9.9877435e-01 1.2233231e-03 2.2259019e-06]\n",
      " [9.9853945e-01 1.4555643e-03 4.9743608e-06]\n",
      " [9.9750829e-01 2.4859393e-03 5.7065417e-06]\n",
      " [9.9825209e-01 1.7418733e-03 6.1184119e-06]\n",
      " [9.9876803e-01 1.2241689e-03 7.7576733e-06]\n",
      " [9.9372733e-01 6.2496774e-03 2.3004381e-05]\n",
      " [9.9254882e-01 7.4213268e-03 2.9814451e-05]\n",
      " [9.9483865e-01 5.1432694e-03 1.8015460e-05]\n",
      " [9.9666518e-01 3.3222756e-03 1.2428710e-05]\n",
      " [9.9833375e-01 1.6614269e-03 4.8929501e-06]\n",
      " [9.9841940e-01 1.5758782e-03 4.7809403e-06]\n",
      " [9.9558049e-01 4.3994901e-03 2.0020771e-05]\n",
      " [9.9525189e-01 4.7286428e-03 1.9459834e-05]\n",
      " [9.9800330e-01 1.9914981e-03 5.2423043e-06]\n",
      " [9.9912530e-01 8.7205024e-04 2.6109599e-06]\n",
      " [9.9944192e-01 5.5670121e-04 1.3513924e-06]\n",
      " [9.9645787e-01 3.5284129e-03 1.3607077e-05]\n",
      " [9.9829584e-01 1.6975536e-03 6.6124835e-06]\n",
      " [9.9905378e-01 9.4390841e-04 2.2534232e-06]\n",
      " [9.9841011e-01 1.5835144e-03 6.2932036e-06]\n",
      " [9.9590021e-01 4.0731328e-03 2.6607264e-05]\n",
      " [9.9797148e-01 2.0219882e-03 6.5146623e-06]\n",
      " [9.9841392e-01 1.5797337e-03 6.2544746e-06]\n",
      " [9.8486590e-01 1.5032055e-02 1.0203257e-04]\n",
      " [9.9661851e-01 3.3590994e-03 2.2286562e-05]\n",
      " [9.9582821e-01 4.1544768e-03 1.7317972e-05]\n",
      " [9.9579525e-01 4.1910070e-03 1.3808557e-05]\n",
      " [9.9604201e-01 3.9402037e-03 1.7811444e-05]\n",
      " [9.9839765e-01 1.5972106e-03 5.1078400e-06]\n",
      " [9.9672180e-01 3.2609408e-03 1.7228138e-05]\n",
      " [9.9874496e-01 1.2516070e-03 3.4035472e-06]\n",
      " [9.9791843e-01 2.0740891e-03 7.5508178e-06]\n",
      " [6.1377423e-04 9.9418676e-01 5.1994105e-03]\n",
      " [1.1380583e-03 9.7895175e-01 1.9910211e-02]\n",
      " [5.9336465e-04 9.6687841e-01 3.2528244e-02]\n",
      " [2.2514444e-03 8.8665676e-01 1.1109184e-01]\n",
      " [8.9729775e-04 9.3879509e-01 6.0307626e-02]\n",
      " [1.5589726e-03 8.5231435e-01 1.4612667e-01]\n",
      " [1.1410990e-03 9.2276263e-01 7.6096326e-02]\n",
      " [1.0408878e-02 9.7581375e-01 1.3777396e-02]\n",
      " [7.9231843e-04 9.8536682e-01 1.3840859e-02]\n",
      " [3.4546666e-03 8.9223850e-01 1.0430678e-01]\n",
      " [3.9318213e-03 9.6559119e-01 3.0476997e-02]\n",
      " [1.9257786e-03 9.5911437e-01 3.8959928e-02]\n",
      " [1.5741860e-03 9.8638701e-01 1.2038906e-02]\n",
      " [1.0826694e-03 8.6373800e-01 1.3517936e-01]\n",
      " [8.7220501e-03 9.8241431e-01 8.8635944e-03]\n",
      " [1.0907879e-03 9.9356502e-01 5.3442214e-03]\n",
      " [1.6636481e-03 7.3043478e-01 2.6790163e-01]\n",
      " [1.9427296e-03 9.8889995e-01 9.1573196e-03]\n",
      " [8.0984586e-04 6.8214399e-01 3.1704617e-01]\n",
      " [2.2614084e-03 9.8384017e-01 1.3898333e-02]\n",
      " [7.5288891e-04 4.1191077e-01 5.8733636e-01]\n",
      " [2.0776454e-03 9.9023879e-01 7.6836459e-03]\n",
      " [4.9336546e-04 5.2050853e-01 4.7899812e-01]\n",
      " [1.0358967e-03 9.2795527e-01 7.1008921e-02]\n",
      " [1.1615072e-03 9.9143267e-01 7.4057453e-03]\n",
      " [9.6914801e-04 9.9159253e-01 7.4382983e-03]\n",
      " [6.1288342e-04 9.6221280e-01 3.7174255e-02]\n",
      " [5.7585287e-04 7.5691801e-01 2.4250613e-01]\n",
      " [1.3779051e-03 8.7343341e-01 1.2518865e-01]\n",
      " [1.2979860e-02 9.8038965e-01 6.6305264e-03]\n",
      " [2.5185887e-03 9.8220348e-01 1.5277996e-02]\n",
      " [3.4170304e-03 9.8602504e-01 1.0557974e-02]\n",
      " [2.5096603e-03 9.8823094e-01 9.2594838e-03]\n",
      " [1.9502632e-04 1.5973985e-01 8.4006512e-01]\n",
      " [1.6274318e-03 5.9929430e-01 3.9907819e-01]\n",
      " [1.6820105e-03 9.3657595e-01 6.1742075e-02]\n",
      " [7.8146654e-04 9.7429943e-01 2.4919160e-02]\n",
      " [9.8021119e-04 9.3737620e-01 6.1643500e-02]\n",
      " [2.4131411e-03 9.7380912e-01 2.3777789e-02]\n",
      " [2.4293559e-03 9.2754155e-01 7.0029080e-02]\n",
      " [1.7427329e-03 8.2778627e-01 1.7047106e-01]\n",
      " [1.2341830e-03 9.2597109e-01 7.2794706e-02]\n",
      " [1.9112478e-03 9.8357242e-01 1.4516350e-02]\n",
      " [8.9332210e-03 9.7758591e-01 1.3480917e-02]\n",
      " [2.0842394e-03 9.2345315e-01 7.4462585e-02]\n",
      " [2.0268373e-03 9.8103285e-01 1.6940361e-02]\n",
      " [2.0535975e-03 9.6321261e-01 3.4733761e-02]\n",
      " [1.2939493e-03 9.8633146e-01 1.2374536e-02]\n",
      " [3.9018553e-02 9.5052993e-01 1.0451508e-02]\n",
      " [2.1292276e-03 9.6875048e-01 2.9120352e-02]\n",
      " [1.1463763e-06 9.9795719e-04 9.9900097e-01]\n",
      " [4.0449333e-05 2.3902934e-02 9.7605658e-01]\n",
      " [1.2392750e-05 2.8639026e-02 9.7134858e-01]\n",
      " [2.7348435e-05 3.2172907e-02 9.6779966e-01]\n",
      " [3.8528692e-06 4.7611948e-03 9.9523491e-01]\n",
      " [1.2511215e-06 6.1883884e-03 9.9381036e-01]\n",
      " [1.5053122e-04 3.4901112e-02 9.6494842e-01]\n",
      " [7.7299228e-06 3.0216498e-02 9.6977580e-01]\n",
      " [8.5187712e-06 1.6579410e-02 9.8341203e-01]\n",
      " [5.2535579e-06 1.0833497e-02 9.8916131e-01]\n",
      " [3.1312727e-04 3.0141869e-01 6.9826812e-01]\n",
      " [5.2663949e-05 5.7821020e-02 9.4212633e-01]\n",
      " [4.3536715e-05 6.4313442e-02 9.3564308e-01]\n",
      " [2.0350706e-05 1.0318997e-02 9.8966062e-01]\n",
      " [7.9567490e-06 3.5073948e-03 9.9648464e-01]\n",
      " [3.6583795e-05 3.0411275e-02 9.6955216e-01]\n",
      " [8.1314807e-05 1.0726063e-01 8.9265811e-01]\n",
      " [5.2021105e-06 2.4720291e-02 9.7527450e-01]\n",
      " [1.0293125e-07 5.6406413e-04 9.9943584e-01]\n",
      " [1.3332191e-04 1.1849694e-01 8.8136977e-01]\n",
      " [1.5378679e-05 2.3844711e-02 9.7613996e-01]\n",
      " [5.8872367e-05 2.4926325e-02 9.7501481e-01]\n",
      " [9.5574944e-07 5.7690274e-03 9.9423003e-01]\n",
      " [3.3699247e-04 2.9650301e-01 7.0315999e-01]\n",
      " [3.0846772e-05 4.3669026e-02 9.5630008e-01]\n",
      " [5.1816187e-05 1.5428936e-01 8.4565878e-01]\n",
      " [5.0629239e-04 3.8366336e-01 6.1583036e-01]\n",
      " [4.7075137e-04 3.3161980e-01 6.6790950e-01]\n",
      " [7.1866921e-06 8.0650263e-03 9.9192774e-01]\n",
      " [1.3198629e-04 4.0885019e-01 5.9101772e-01]\n",
      " [1.2166154e-05 4.5756627e-02 9.5423114e-01]\n",
      " [5.6639972e-05 3.0913609e-01 6.9080728e-01]\n",
      " [4.7382887e-06 5.0495979e-03 9.9494570e-01]\n",
      " [4.4644187e-04 4.9286902e-01 5.0668454e-01]\n",
      " [4.4414279e-05 5.6114201e-02 9.4384134e-01]\n",
      " [7.0244259e-06 2.7279461e-02 9.7271347e-01]\n",
      " [8.3780242e-06 6.7114513e-03 9.9328023e-01]\n",
      " [8.6619460e-05 1.0303124e-01 8.9688218e-01]\n",
      " [5.8847957e-04 3.6261585e-01 6.3679570e-01]\n",
      " [1.0261818e-04 1.5566559e-01 8.4423178e-01]\n",
      " [8.2065144e-06 9.8453900e-03 9.9014634e-01]\n",
      " [1.7167589e-04 2.0523477e-01 7.9459351e-01]\n",
      " [4.0449333e-05 2.3902934e-02 9.7605658e-01]\n",
      " [4.8399597e-06 7.5337165e-03 9.9246150e-01]\n",
      " [5.6428289e-06 6.5352614e-03 9.9345917e-01]\n",
      " [6.2549610e-05 6.6688724e-02 9.3324876e-01]\n",
      " [1.0177440e-04 9.1969341e-02 9.0792888e-01]\n",
      " [1.3856863e-04 1.4404865e-01 8.5581285e-01]\n",
      " [2.5735677e-05 1.8213369e-02 9.8176092e-01]\n",
      " [1.5860714e-04 1.0272487e-01 8.9711648e-01]]\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(x)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1]], dtype=uint8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_classes = np.argmax(pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_classes = np.argmax(y,axis=1)\n",
    "expected_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'expected_classes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9a1fb96aff5f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcorrect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpected_classes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'expected_classes' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "correct = accuracy_score(expected_classes,predict_classes)\n",
    "correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading Trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 0s - loss: 1520412.9746\n",
      "Epoch 2/100\n",
      " - 0s - loss: 524035.4553\n",
      "Epoch 3/100\n",
      " - 0s - loss: 96374.6054\n",
      "Epoch 4/100\n",
      " - 0s - loss: 3601.4125\n",
      "Epoch 5/100\n",
      " - 0s - loss: 3945.8596\n",
      "Epoch 6/100\n",
      " - 0s - loss: 2382.5820\n",
      "Epoch 7/100\n",
      " - 0s - loss: 315.6235\n",
      "Epoch 8/100\n",
      " - 0s - loss: 256.9239\n",
      "Epoch 9/100\n",
      " - 0s - loss: 223.3152\n",
      "Epoch 10/100\n",
      " - 0s - loss: 184.8626\n",
      "Epoch 11/100\n",
      " - 0s - loss: 183.7058\n",
      "Epoch 12/100\n",
      " - 0s - loss: 180.2825\n",
      "Epoch 13/100\n",
      " - 0s - loss: 178.6125\n",
      "Epoch 14/100\n",
      " - 0s - loss: 177.3656\n",
      "Epoch 15/100\n",
      " - 0s - loss: 176.4439\n",
      "Epoch 16/100\n",
      " - 0s - loss: 175.8701\n",
      "Epoch 17/100\n",
      " - 0s - loss: 173.4551\n",
      "Epoch 18/100\n",
      " - 0s - loss: 172.0698\n",
      "Epoch 19/100\n",
      " - 0s - loss: 170.4598\n",
      "Epoch 20/100\n",
      " - 0s - loss: 169.3883\n",
      "Epoch 21/100\n",
      " - 0s - loss: 168.0756\n",
      "Epoch 22/100\n",
      " - 0s - loss: 166.3506\n",
      "Epoch 23/100\n",
      " - 0s - loss: 164.9401\n",
      "Epoch 24/100\n",
      " - 0s - loss: 163.3923\n",
      "Epoch 25/100\n",
      " - 0s - loss: 161.8938\n",
      "Epoch 26/100\n",
      " - 0s - loss: 160.3665\n",
      "Epoch 27/100\n",
      " - 0s - loss: 159.3668\n",
      "Epoch 28/100\n",
      " - 0s - loss: 157.3250\n",
      "Epoch 29/100\n",
      " - 0s - loss: 155.8630\n",
      "Epoch 30/100\n",
      " - 0s - loss: 154.3399\n",
      "Epoch 31/100\n",
      " - 0s - loss: 153.2440\n",
      "Epoch 32/100\n",
      " - 0s - loss: 152.1323\n",
      "Epoch 33/100\n",
      " - 0s - loss: 150.5069\n",
      "Epoch 34/100\n",
      " - 0s - loss: 148.7333\n",
      "Epoch 35/100\n",
      " - 0s - loss: 147.2913\n",
      "Epoch 36/100\n",
      " - 0s - loss: 145.6333\n",
      "Epoch 37/100\n",
      " - 0s - loss: 144.7763\n",
      "Epoch 38/100\n",
      " - 0s - loss: 142.4260\n",
      "Epoch 39/100\n",
      " - 0s - loss: 140.5751\n",
      "Epoch 40/100\n",
      " - 0s - loss: 139.7374\n",
      "Epoch 41/100\n",
      " - 0s - loss: 137.9179\n",
      "Epoch 42/100\n",
      " - 0s - loss: 136.9757\n",
      "Epoch 43/100\n",
      " - 0s - loss: 135.1905\n",
      "Epoch 44/100\n",
      " - 0s - loss: 134.3770\n",
      "Epoch 45/100\n",
      " - 0s - loss: 131.8869\n",
      "Epoch 46/100\n",
      " - 0s - loss: 130.4829\n",
      "Epoch 47/100\n",
      " - 0s - loss: 129.1369\n",
      "Epoch 48/100\n",
      " - 0s - loss: 127.8791\n",
      "Epoch 49/100\n",
      " - 0s - loss: 126.7119\n",
      "Epoch 50/100\n",
      " - 0s - loss: 125.7624\n",
      "Epoch 51/100\n",
      " - 0s - loss: 124.2138\n",
      "Epoch 52/100\n",
      " - 0s - loss: 123.9281\n",
      "Epoch 53/100\n",
      " - 0s - loss: 121.0088\n",
      "Epoch 54/100\n",
      " - 0s - loss: 119.9720\n",
      "Epoch 55/100\n",
      " - 0s - loss: 118.9121\n",
      "Epoch 56/100\n",
      " - 0s - loss: 117.5765\n",
      "Epoch 57/100\n",
      " - 0s - loss: 117.1468\n",
      "Epoch 58/100\n",
      " - 0s - loss: 114.8635\n",
      "Epoch 59/100\n",
      " - 0s - loss: 113.8351\n",
      "Epoch 60/100\n",
      " - 0s - loss: 112.7251\n",
      "Epoch 61/100\n",
      " - 0s - loss: 111.4283\n",
      "Epoch 62/100\n",
      " - 0s - loss: 110.1523\n",
      "Epoch 63/100\n",
      " - 0s - loss: 110.3647\n",
      "Epoch 64/100\n",
      " - 0s - loss: 108.0103\n",
      "Epoch 65/100\n",
      " - 0s - loss: 107.4184\n",
      "Epoch 66/100\n",
      " - 0s - loss: 105.5738\n",
      "Epoch 67/100\n",
      " - 0s - loss: 104.8296\n",
      "Epoch 68/100\n",
      " - 0s - loss: 103.3584\n",
      "Epoch 69/100\n",
      " - 0s - loss: 102.7059\n",
      "Epoch 70/100\n",
      " - 0s - loss: 101.9387\n",
      "Epoch 71/100\n",
      " - 0s - loss: 100.3313\n",
      "Epoch 72/100\n",
      " - 0s - loss: 99.8766\n",
      "Epoch 73/100\n",
      " - 0s - loss: 98.6233\n",
      "Epoch 74/100\n",
      " - 0s - loss: 98.4569\n",
      "Epoch 75/100\n",
      " - 0s - loss: 97.6709\n",
      "Epoch 76/100\n",
      " - 0s - loss: 96.4635\n",
      "Epoch 77/100\n",
      " - 0s - loss: 95.9738\n",
      "Epoch 78/100\n",
      " - 0s - loss: 95.2730\n",
      "Epoch 79/100\n",
      " - 0s - loss: 93.3181\n",
      "Epoch 80/100\n",
      " - 0s - loss: 92.9768\n",
      "Epoch 81/100\n",
      " - 0s - loss: 91.8636\n",
      "Epoch 82/100\n",
      " - 0s - loss: 91.1641\n",
      "Epoch 83/100\n",
      " - 0s - loss: 90.3111\n",
      "Epoch 84/100\n",
      " - 0s - loss: 90.4785\n",
      "Epoch 85/100\n",
      " - 0s - loss: 89.0106\n",
      "Epoch 86/100\n",
      " - 0s - loss: 88.7591\n",
      "Epoch 87/100\n",
      " - 0s - loss: 87.8856\n",
      "Epoch 88/100\n",
      " - 0s - loss: 86.5107\n",
      "Epoch 89/100\n",
      " - 0s - loss: 85.9148\n",
      "Epoch 90/100\n",
      " - 0s - loss: 85.5948\n",
      "Epoch 91/100\n",
      " - 0s - loss: 84.2605\n",
      "Epoch 92/100\n",
      " - 0s - loss: 84.1035\n",
      "Epoch 93/100\n",
      " - 0s - loss: 84.1716\n",
      "Epoch 94/100\n",
      " - 0s - loss: 82.1387\n",
      "Epoch 95/100\n",
      " - 0s - loss: 82.4723\n",
      "Epoch 96/100\n",
      " - 0s - loss: 81.5648\n",
      "Epoch 97/100\n",
      " - 0s - loss: 80.5444\n",
      "Epoch 98/100\n",
      " - 0s - loss: 80.2400\n",
      "Epoch 99/100\n",
      " - 0s - loss: 79.8919\n",
      "Epoch 100/100\n",
      " - 0s - loss: 79.5353\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "import pandas as pd\n",
    "import io\n",
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "path = './data/'\n",
    "\n",
    "\n",
    "read_csv = os.path.join(path, \"auto-mpg.csv\")\n",
    "\n",
    "\n",
    "df = pd.read_csv(read_csv , na_values = ['NA' , '?'])\n",
    "\n",
    "cars = df['name']\n",
    "df['horsepower'] = df['horsepower'].fillna(df['horsepower'].median())\n",
    "\n",
    "x = df[['cylinders', 'displacement', 'horsepower', 'weight',\n",
    "       'acceleration', 'year', 'origin']].values\n",
    "y = df['mpg'].values\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=x.shape[1], activation='relu')) # Hidden 1\n",
    "model.add(Dense(10, activation='relu')) # Hidden 2\n",
    "model.add(Dense(1)) # Output\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(x,y,verbose=2,epochs=100)\n",
    "\n",
    "\n",
    "pred = model.predict(x)\n",
    "\n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y))\n",
    "\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(os.path.join(path,\"network.json\"), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "model.save(os.path.join(path,\"network.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 16.214561 ],\n",
       "       [  6.942596 ],\n",
       "       [  8.82758  ],\n",
       "       [  9.005185 ],\n",
       "       [ 12.838344 ],\n",
       "       [  4.4987226],\n",
       "       [ -2.7331436],\n",
       "       [ -1.2085617],\n",
       "       [ -3.4762628],\n",
       "       [  1.0578606],\n",
       "       [  2.584483 ],\n",
       "       [  8.485381 ],\n",
       "       [ 11.429406 ],\n",
       "       [-24.527317 ],\n",
       "       [ 17.476858 ],\n",
       "       [ 20.756487 ],\n",
       "       [ 19.173342 ],\n",
       "       [ 19.781063 ],\n",
       "       [ 16.548489 ],\n",
       "       [ 23.545872 ],\n",
       "       [ 24.352669 ],\n",
       "       [ 20.545454 ],\n",
       "       [ 17.416794 ],\n",
       "       [ 10.382931 ],\n",
       "       [ 19.499193 ],\n",
       "       [  4.8973246],\n",
       "       [  7.402699 ],\n",
       "       [  4.4160156],\n",
       "       [ 14.3190775],\n",
       "       [ 16.852203 ],\n",
       "       [ 17.048014 ],\n",
       "       [ 15.790214 ],\n",
       "       [ 13.072803 ],\n",
       "       [ 15.855496 ],\n",
       "       [ 26.526707 ],\n",
       "       [ 25.457525 ],\n",
       "       [ 28.753094 ],\n",
       "       [ 25.45371  ],\n",
       "       [ 15.221793 ],\n",
       "       [ 14.487981 ],\n",
       "       [ 17.631231 ],\n",
       "       [ 18.960686 ],\n",
       "       [ 21.262371 ],\n",
       "       [ 20.334919 ],\n",
       "       [ 24.978867 ],\n",
       "       [ 16.849998 ],\n",
       "       [ 23.934381 ],\n",
       "       [ 24.855482 ],\n",
       "       [ 26.464159 ],\n",
       "       [ 18.647196 ],\n",
       "       [ 15.839216 ],\n",
       "       [ 21.081112 ],\n",
       "       [ 20.127144 ],\n",
       "       [ 18.063982 ],\n",
       "       [ 14.546494 ],\n",
       "       [ 19.904165 ],\n",
       "       [ 18.766056 ],\n",
       "       [ 16.46977  ],\n",
       "       [ 19.384401 ],\n",
       "       [ 27.440042 ],\n",
       "       [ 18.521446 ],\n",
       "       [ 18.361406 ],\n",
       "       [ 16.54733  ],\n",
       "       [ 13.413143 ],\n",
       "       [ 19.740807 ],\n",
       "       [ 17.678755 ],\n",
       "       [ 13.506999 ],\n",
       "       [  6.323633 ],\n",
       "       [ 22.821754 ],\n",
       "       [ 20.548353 ],\n",
       "       [  9.207922 ],\n",
       "       [ 19.404325 ],\n",
       "       [ 16.69224  ],\n",
       "       [ 25.61376  ],\n",
       "       [ 25.22586  ],\n",
       "       [ 18.69198  ],\n",
       "       [ 21.742619 ],\n",
       "       [ 25.310722 ],\n",
       "       [ 28.883852 ],\n",
       "       [ 23.308292 ],\n",
       "       [ 21.155046 ],\n",
       "       [ 17.716175 ],\n",
       "       [ 19.45881  ],\n",
       "       [ 20.51108  ],\n",
       "       [ 16.137178 ],\n",
       "       [ 10.742244 ],\n",
       "       [ 13.810912 ],\n",
       "       [ 18.281452 ],\n",
       "       [ 22.908312 ],\n",
       "       [ 14.689897 ],\n",
       "       [ 14.60542  ],\n",
       "       [ 22.7093   ],\n",
       "       [ 20.111502 ],\n",
       "       [ 21.374584 ],\n",
       "       [  5.669135 ],\n",
       "       [  5.4312754],\n",
       "       [  6.547121 ],\n",
       "       [ 21.861296 ],\n",
       "       [ 24.579496 ],\n",
       "       [ 20.531954 ],\n",
       "       [ 24.66943  ],\n",
       "       [ 22.64723  ],\n",
       "       [ 26.128487 ],\n",
       "       [ 30.541412 ],\n",
       "       [ 24.251062 ],\n",
       "       [ 20.64599  ],\n",
       "       [ 15.60174  ],\n",
       "       [ 18.352783 ],\n",
       "       [ 18.571125 ],\n",
       "       [ 24.29519  ],\n",
       "       [ 18.575813 ],\n",
       "       [ 18.639494 ],\n",
       "       [ 19.748644 ],\n",
       "       [ 14.213085 ],\n",
       "       [ 18.909533 ],\n",
       "       [ 19.7593   ],\n",
       "       [ -4.3316107],\n",
       "       [ 25.33593  ],\n",
       "       [ 21.236803 ],\n",
       "       [ 23.424295 ],\n",
       "       [ 20.750921 ],\n",
       "       [  9.157421 ],\n",
       "       [ 18.511248 ],\n",
       "       [ 14.417838 ],\n",
       "       [  2.8847215],\n",
       "       [ 25.927244 ],\n",
       "       [ 22.615423 ],\n",
       "       [ 20.143986 ],\n",
       "       [ 26.068632 ],\n",
       "       [ 20.853476 ],\n",
       "       [ 24.366404 ],\n",
       "       [ 19.418655 ],\n",
       "       [ 26.569687 ],\n",
       "       [ 33.065014 ],\n",
       "       [ 27.0641   ],\n",
       "       [ 29.899992 ],\n",
       "       [ 23.975443 ],\n",
       "       [ 27.802767 ],\n",
       "       [ 25.410707 ],\n",
       "       [ 31.241789 ],\n",
       "       [ 22.217707 ],\n",
       "       [ 20.386671 ],\n",
       "       [ 22.271742 ],\n",
       "       [ 23.792767 ],\n",
       "       [ 21.562164 ],\n",
       "       [ 23.404383 ],\n",
       "       [ 22.477201 ],\n",
       "       [ 21.679901 ],\n",
       "       [ 23.334608 ],\n",
       "       [ 19.662344 ],\n",
       "       [ 19.651907 ],\n",
       "       [ 22.716764 ],\n",
       "       [ 27.954046 ],\n",
       "       [ 27.029716 ],\n",
       "       [ 35.46784  ],\n",
       "       [ 31.570229 ],\n",
       "       [ 20.460445 ],\n",
       "       [ 25.721632 ],\n",
       "       [ 26.085634 ],\n",
       "       [ 28.304483 ],\n",
       "       [ 31.831398 ],\n",
       "       [ 33.231934 ],\n",
       "       [ 28.634966 ],\n",
       "       [ 35.32414  ],\n",
       "       [ 19.826523 ],\n",
       "       [ 20.67592  ],\n",
       "       [ 12.955035 ],\n",
       "       [ 22.331886 ],\n",
       "       [ 25.918478 ],\n",
       "       [ 20.652096 ],\n",
       "       [ 26.319235 ],\n",
       "       [ 23.537054 ],\n",
       "       [ 24.758198 ],\n",
       "       [ 20.334978 ],\n",
       "       [ 25.264046 ],\n",
       "       [ 21.255787 ],\n",
       "       [ 28.147858 ],\n",
       "       [ 24.243446 ],\n",
       "       [ 29.823145 ],\n",
       "       [ 27.182734 ],\n",
       "       [ 17.87856  ],\n",
       "       [ 23.043844 ],\n",
       "       [ 23.869207 ],\n",
       "       [ 20.879921 ],\n",
       "       [ 22.953426 ],\n",
       "       [ 22.728453 ],\n",
       "       [ 20.948563 ],\n",
       "       [ 25.913252 ],\n",
       "       [ 21.957438 ],\n",
       "       [ 27.92523  ],\n",
       "       [ 20.610773 ],\n",
       "       [ 26.384302 ],\n",
       "       [ 26.077612 ],\n",
       "       [ 29.087952 ],\n",
       "       [ 26.306759 ],\n",
       "       [ 26.871435 ],\n",
       "       [ 25.984148 ],\n",
       "       [ 21.504807 ],\n",
       "       [ 23.37497  ],\n",
       "       [ 32.326714 ],\n",
       "       [ 36.14387  ],\n",
       "       [ 28.653097 ],\n",
       "       [ 25.474634 ],\n",
       "       [ 19.731993 ],\n",
       "       [ 21.493452 ],\n",
       "       [ 22.274612 ],\n",
       "       [ 29.399178 ],\n",
       "       [ 28.821009 ],\n",
       "       [ 17.97241  ],\n",
       "       [ 33.70738  ],\n",
       "       [ 21.05589  ],\n",
       "       [ 30.829437 ],\n",
       "       [ 14.751782 ],\n",
       "       [ 20.51985  ],\n",
       "       [ 23.148989 ],\n",
       "       [ 14.844945 ],\n",
       "       [ 22.40904  ],\n",
       "       [ 21.480791 ],\n",
       "       [ 22.955597 ],\n",
       "       [ 18.217348 ],\n",
       "       [ 21.144617 ],\n",
       "       [ 19.53716  ],\n",
       "       [ 33.040356 ],\n",
       "       [ 22.833597 ],\n",
       "       [ 30.161726 ],\n",
       "       [ 26.936995 ],\n",
       "       [ 27.53276  ],\n",
       "       [ 32.3005   ],\n",
       "       [ 30.02418  ],\n",
       "       [ 11.034275 ],\n",
       "       [ 14.967047 ],\n",
       "       [  9.284125 ],\n",
       "       [ 23.266222 ],\n",
       "       [ 19.044542 ],\n",
       "       [ 26.445995 ],\n",
       "       [ 23.815369 ],\n",
       "       [ 26.818071 ],\n",
       "       [ 24.97674  ],\n",
       "       [ 19.454733 ],\n",
       "       [ 22.385761 ],\n",
       "       [ 23.084352 ],\n",
       "       [ 23.59139  ],\n",
       "       [ 19.111555 ],\n",
       "       [ 22.66872  ],\n",
       "       [ 27.690014 ],\n",
       "       [ 21.115692 ],\n",
       "       [ 27.195122 ],\n",
       "       [ 22.92105  ],\n",
       "       [ 22.164698 ],\n",
       "       [ 23.37523  ],\n",
       "       [ 18.457056 ],\n",
       "       [ 16.851906 ],\n",
       "       [ 28.936792 ],\n",
       "       [ 27.439524 ],\n",
       "       [ 28.209082 ],\n",
       "       [ 26.991007 ],\n",
       "       [ 29.59686  ],\n",
       "       [ 28.988974 ],\n",
       "       [ 27.430143 ],\n",
       "       [ 29.61341  ],\n",
       "       [ 29.073496 ],\n",
       "       [ 22.482323 ],\n",
       "       [ 12.496005 ],\n",
       "       [ 10.508229 ],\n",
       "       [ 11.551061 ],\n",
       "       [ 23.744165 ],\n",
       "       [ 25.502264 ],\n",
       "       [ 22.334455 ],\n",
       "       [ 18.023832 ],\n",
       "       [ 24.809328 ],\n",
       "       [ 21.462805 ],\n",
       "       [ 21.18898  ],\n",
       "       [ 29.050068 ],\n",
       "       [ 19.619896 ],\n",
       "       [ 23.337822 ],\n",
       "       [ 20.22211  ],\n",
       "       [ 20.137726 ],\n",
       "       [ 21.385233 ],\n",
       "       [ 22.47964  ],\n",
       "       [ 24.647566 ],\n",
       "       [ 22.620897 ],\n",
       "       [ 28.249273 ],\n",
       "       [ 29.44759  ],\n",
       "       [ 29.883696 ],\n",
       "       [ 25.864069 ],\n",
       "       [ 23.372595 ],\n",
       "       [ 22.528574 ],\n",
       "       [ 21.665398 ],\n",
       "       [ 21.257269 ],\n",
       "       [ 22.332907 ],\n",
       "       [ 21.680555 ],\n",
       "       [ 22.689896 ],\n",
       "       [ 17.443632 ],\n",
       "       [ 22.00788  ],\n",
       "       [ 24.176043 ],\n",
       "       [ 18.887321 ],\n",
       "       [ 29.774452 ],\n",
       "       [ 39.70737  ],\n",
       "       [ 23.71742  ],\n",
       "       [ 37.090466 ],\n",
       "       [ 28.910727 ],\n",
       "       [ 26.547174 ],\n",
       "       [ 25.29608  ],\n",
       "       [ 23.82477  ],\n",
       "       [ 25.588217 ],\n",
       "       [ 25.333252 ],\n",
       "       [ 15.5875845],\n",
       "       [ 16.800722 ],\n",
       "       [ 24.306843 ],\n",
       "       [ 23.692934 ],\n",
       "       [ 24.828127 ],\n",
       "       [ 25.21317  ],\n",
       "       [ 24.84334  ],\n",
       "       [ 25.625807 ],\n",
       "       [ 29.218145 ],\n",
       "       [ 29.750427 ],\n",
       "       [ 32.123722 ],\n",
       "       [ 23.499268 ],\n",
       "       [ 26.510393 ],\n",
       "       [ 28.454535 ],\n",
       "       [ 22.206114 ],\n",
       "       [ 25.156055 ],\n",
       "       [ 25.863558 ],\n",
       "       [ 23.290369 ],\n",
       "       [ 25.543493 ],\n",
       "       [ 29.815119 ],\n",
       "       [ 33.198368 ],\n",
       "       [ 36.253704 ],\n",
       "       [ 40.22046  ],\n",
       "       [ 22.099714 ],\n",
       "       [ 13.161219 ],\n",
       "       [ 25.37491  ],\n",
       "       [ 23.488348 ],\n",
       "       [ 15.211326 ],\n",
       "       [ 22.593372 ],\n",
       "       [ 24.606056 ],\n",
       "       [ 29.102976 ],\n",
       "       [ 26.02223  ],\n",
       "       [ 25.622326 ],\n",
       "       [ 27.14125  ],\n",
       "       [ 24.794504 ],\n",
       "       [ 19.433449 ],\n",
       "       [ 24.737644 ],\n",
       "       [ 23.278927 ],\n",
       "       [ 23.70603  ],\n",
       "       [ 22.885178 ],\n",
       "       [ 24.475744 ],\n",
       "       [ 23.6702   ],\n",
       "       [ 26.211443 ],\n",
       "       [ 23.613941 ],\n",
       "       [ 29.095537 ],\n",
       "       [ 25.696386 ],\n",
       "       [ 29.73194  ],\n",
       "       [ 25.227667 ],\n",
       "       [ 20.966393 ],\n",
       "       [ 24.849657 ],\n",
       "       [ 26.358536 ],\n",
       "       [ 22.930187 ],\n",
       "       [ 30.311666 ],\n",
       "       [ 36.740612 ],\n",
       "       [ 35.63755  ],\n",
       "       [ 19.989647 ],\n",
       "       [ 19.676018 ],\n",
       "       [ 27.341812 ],\n",
       "       [ 27.335833 ],\n",
       "       [ 29.328323 ],\n",
       "       [ 35.872643 ],\n",
       "       [ 26.245714 ],\n",
       "       [ 27.069563 ],\n",
       "       [ 23.381907 ],\n",
       "       [ 27.634157 ],\n",
       "       [ 26.394272 ],\n",
       "       [ 26.719309 ],\n",
       "       [ 28.972227 ],\n",
       "       [ 29.66685  ],\n",
       "       [ 21.928904 ],\n",
       "       [ 23.944729 ],\n",
       "       [ 23.244267 ],\n",
       "       [ 28.03915  ],\n",
       "       [ 25.406996 ],\n",
       "       [ 19.847143 ],\n",
       "       [ 25.0474   ],\n",
       "       [ 26.534208 ],\n",
       "       [ 24.186935 ],\n",
       "       [ 23.995476 ],\n",
       "       [ 24.330301 ],\n",
       "       [ 21.87162  ],\n",
       "       [ 27.67553  ],\n",
       "       [ 24.52055  ],\n",
       "       [ 18.172358 ],\n",
       "       [ 24.616657 ],\n",
       "       [ 24.77824  ],\n",
       "       [ 30.290895 ],\n",
       "       [ 29.872137 ],\n",
       "       [ 28.847685 ],\n",
       "       [ 23.982185 ],\n",
       "       [ 29.339052 ],\n",
       "       [ 29.719387 ]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model2 = load_model(os.path.join(path,\"network.h5\"))\n",
    "pred = model2.predict(x)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 298 samples, validate on 100 samples\n",
      "Epoch 1/1000\n",
      " - 0s - loss: 1108.5536 - val_loss: 841.0391\n",
      "Epoch 2/1000\n",
      " - 0s - loss: 662.0666 - val_loss: 498.1523\n",
      "Epoch 3/1000\n",
      " - 0s - loss: 506.7915 - val_loss: 445.7672\n",
      "Epoch 4/1000\n",
      " - 0s - loss: 435.2467 - val_loss: 426.7556\n",
      "Epoch 5/1000\n",
      " - 0s - loss: 417.4484 - val_loss: 412.9195\n",
      "Epoch 6/1000\n",
      " - 0s - loss: 382.8235 - val_loss: 390.8029\n",
      "Epoch 7/1000\n",
      " - 0s - loss: 372.6940 - val_loss: 372.5941\n",
      "Epoch 8/1000\n",
      " - 0s - loss: 345.8538 - val_loss: 357.0759\n",
      "Epoch 9/1000\n",
      " - 0s - loss: 330.2376 - val_loss: 339.3506\n",
      "Epoch 10/1000\n",
      " - 0s - loss: 324.5691 - val_loss: 326.2807\n",
      "Epoch 11/1000\n",
      " - 0s - loss: 305.6194 - val_loss: 310.3591\n",
      "Epoch 12/1000\n",
      " - 0s - loss: 291.6372 - val_loss: 296.9478\n",
      "Epoch 13/1000\n",
      " - 0s - loss: 277.9849 - val_loss: 281.0843\n",
      "Epoch 14/1000\n",
      " - 0s - loss: 271.0921 - val_loss: 267.8845\n",
      "Epoch 15/1000\n",
      " - 0s - loss: 254.1218 - val_loss: 254.7186\n",
      "Epoch 16/1000\n",
      " - 0s - loss: 242.0048 - val_loss: 243.8319\n",
      "Epoch 17/1000\n",
      " - 0s - loss: 231.3322 - val_loss: 230.8480\n",
      "Epoch 18/1000\n",
      " - 0s - loss: 222.2861 - val_loss: 222.7843\n",
      "Epoch 19/1000\n",
      " - 0s - loss: 212.3271 - val_loss: 207.2870\n",
      "Epoch 20/1000\n",
      " - 0s - loss: 199.1556 - val_loss: 198.8594\n",
      "Epoch 21/1000\n",
      " - 0s - loss: 193.2079 - val_loss: 186.0329\n",
      "Epoch 22/1000\n",
      " - 0s - loss: 182.2627 - val_loss: 182.5523\n",
      "Epoch 23/1000\n",
      " - 0s - loss: 188.6476 - val_loss: 167.5952\n",
      "Epoch 24/1000\n",
      " - 0s - loss: 169.3742 - val_loss: 158.2942\n",
      "Epoch 25/1000\n",
      " - 0s - loss: 157.1120 - val_loss: 148.9276\n",
      "Epoch 26/1000\n",
      " - 0s - loss: 150.4676 - val_loss: 140.6114\n",
      "Epoch 27/1000\n",
      " - 0s - loss: 140.4128 - val_loss: 132.9065\n",
      "Epoch 28/1000\n",
      " - 0s - loss: 135.5155 - val_loss: 134.1232\n",
      "Epoch 29/1000\n",
      " - 0s - loss: 131.4759 - val_loss: 125.4989\n",
      "Epoch 30/1000\n",
      " - 0s - loss: 125.0636 - val_loss: 113.3347\n",
      "Epoch 31/1000\n",
      " - 0s - loss: 113.8834 - val_loss: 106.3689\n",
      "Epoch 32/1000\n",
      " - 0s - loss: 108.9651 - val_loss: 98.8869\n",
      "Epoch 33/1000\n",
      " - 0s - loss: 102.8333 - val_loss: 93.4960\n",
      "Epoch 34/1000\n",
      " - 0s - loss: 97.8609 - val_loss: 90.1099\n",
      "Epoch 35/1000\n",
      " - 0s - loss: 93.3451 - val_loss: 82.1344\n",
      "Epoch 36/1000\n",
      " - 0s - loss: 88.2168 - val_loss: 77.1884\n",
      "Epoch 37/1000\n",
      " - 0s - loss: 83.3290 - val_loss: 72.1665\n",
      "Epoch 38/1000\n",
      " - 0s - loss: 79.0019 - val_loss: 70.1310\n",
      "Epoch 39/1000\n",
      " - 0s - loss: 80.5763 - val_loss: 63.5990\n",
      "Epoch 40/1000\n",
      " - 0s - loss: 72.1667 - val_loss: 60.1474\n",
      "Epoch 41/1000\n",
      " - 0s - loss: 67.8882 - val_loss: 55.7291\n",
      "Epoch 42/1000\n",
      " - 0s - loss: 63.7129 - val_loss: 53.6937\n",
      "Epoch 43/1000\n",
      " - 0s - loss: 61.3155 - val_loss: 51.6126\n",
      "Epoch 44/1000\n",
      " - 0s - loss: 57.9847 - val_loss: 47.1997\n",
      "Epoch 45/1000\n",
      " - 0s - loss: 54.9938 - val_loss: 43.1666\n",
      "Epoch 46/1000\n",
      " - 0s - loss: 51.5818 - val_loss: 41.4149\n",
      "Epoch 47/1000\n",
      " - 0s - loss: 48.5462 - val_loss: 38.1640\n",
      "Epoch 48/1000\n",
      " - 0s - loss: 46.2608 - val_loss: 36.8647\n",
      "Epoch 49/1000\n",
      " - 0s - loss: 44.6076 - val_loss: 35.5562\n",
      "Epoch 50/1000\n",
      " - 0s - loss: 42.1090 - val_loss: 32.5583\n",
      "Epoch 51/1000\n",
      " - 0s - loss: 40.8716 - val_loss: 31.4731\n",
      "Epoch 52/1000\n",
      " - 0s - loss: 39.1638 - val_loss: 33.5924\n",
      "Epoch 53/1000\n",
      " - 0s - loss: 37.9404 - val_loss: 27.6376\n",
      "Epoch 54/1000\n",
      " - 0s - loss: 35.7010 - val_loss: 26.4980\n",
      "Epoch 55/1000\n",
      " - 0s - loss: 35.3165 - val_loss: 26.1006\n",
      "Epoch 56/1000\n",
      " - 0s - loss: 33.2780 - val_loss: 29.4900\n",
      "Epoch 57/1000\n",
      " - 0s - loss: 32.1890 - val_loss: 25.5596\n",
      "Epoch 58/1000\n",
      " - 0s - loss: 32.4224 - val_loss: 22.8190\n",
      "Epoch 59/1000\n",
      " - 0s - loss: 31.5722 - val_loss: 31.6935\n",
      "Epoch 60/1000\n",
      " - 0s - loss: 31.3116 - val_loss: 24.0465\n",
      "Epoch 61/1000\n",
      " - 0s - loss: 29.0444 - val_loss: 20.9044\n",
      "Epoch 62/1000\n",
      " - 0s - loss: 27.6606 - val_loss: 19.5745\n",
      "Epoch 63/1000\n",
      " - 0s - loss: 27.0584 - val_loss: 19.0720\n",
      "Epoch 64/1000\n",
      " - 0s - loss: 26.2391 - val_loss: 19.3182\n",
      "Epoch 65/1000\n",
      " - 0s - loss: 25.6626 - val_loss: 19.1637\n",
      "Epoch 66/1000\n",
      " - 0s - loss: 25.0526 - val_loss: 18.8075\n",
      "Epoch 67/1000\n",
      " - 0s - loss: 24.5553 - val_loss: 20.4377\n",
      "Epoch 68/1000\n",
      " - 0s - loss: 24.8470 - val_loss: 17.7916\n",
      "Epoch 69/1000\n",
      " - 0s - loss: 24.5236 - val_loss: 18.4436\n",
      "Epoch 70/1000\n",
      " - 0s - loss: 25.4047 - val_loss: 17.3938\n",
      "Epoch 71/1000\n",
      " - 0s - loss: 25.2858 - val_loss: 17.3090\n",
      "Epoch 72/1000\n",
      " - 0s - loss: 25.6501 - val_loss: 16.7646\n",
      "Epoch 73/1000\n",
      " - 0s - loss: 22.6451 - val_loss: 19.8091\n",
      "Epoch 74/1000\n",
      " - 0s - loss: 23.2963 - val_loss: 20.5425\n",
      "Epoch 75/1000\n",
      " - 0s - loss: 24.8961 - val_loss: 16.6875\n",
      "Epoch 76/1000\n",
      " - 0s - loss: 21.9400 - val_loss: 16.2540\n",
      "Epoch 77/1000\n",
      " - 0s - loss: 21.9142 - val_loss: 19.3987\n",
      "Epoch 78/1000\n",
      " - 0s - loss: 20.7801 - val_loss: 16.0343\n",
      "Epoch 79/1000\n",
      " - 0s - loss: 20.8563 - val_loss: 17.1164\n",
      "Epoch 80/1000\n",
      " - 0s - loss: 20.1211 - val_loss: 16.4666\n",
      "Epoch 81/1000\n",
      " - 0s - loss: 21.1780 - val_loss: 15.2025\n",
      "Epoch 82/1000\n",
      " - 0s - loss: 20.5475 - val_loss: 15.1029\n",
      "Epoch 83/1000\n",
      " - 0s - loss: 20.0652 - val_loss: 15.1984\n",
      "Epoch 84/1000\n",
      " - 0s - loss: 20.5712 - val_loss: 15.3999\n",
      "Epoch 85/1000\n",
      " - 0s - loss: 20.9577 - val_loss: 14.7507\n",
      "Epoch 86/1000\n",
      " - 0s - loss: 19.8113 - val_loss: 16.2563\n",
      "Epoch 87/1000\n",
      " - 0s - loss: 19.6408 - val_loss: 16.9037\n",
      "Epoch 88/1000\n",
      " - 0s - loss: 19.3265 - val_loss: 20.1132\n",
      "Epoch 89/1000\n",
      " - 0s - loss: 19.7363 - val_loss: 15.7125\n",
      "Epoch 90/1000\n",
      " - 0s - loss: 19.6638 - val_loss: 18.8740\n",
      "Epoch 00090: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x28b4852f320>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "import pandas as pd\n",
    "import io\n",
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "path = './data/'\n",
    "\n",
    "\n",
    "read_csv = os.path.join(path, \"auto-mpg.csv\")\n",
    "\n",
    "\n",
    "df = pd.read_csv(read_csv , na_values = ['NA' , '?'])\n",
    "\n",
    "cars = df['name']\n",
    "df['horsepower'] = df['horsepower'].fillna(df['horsepower'].median())\n",
    "\n",
    "x = df[['cylinders', 'displacement', 'horsepower', 'weight','acceleration', 'year', 'origin']].values\n",
    "y = df['mpg'].values\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=x.shape[1], activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1)) # Output\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto',restore_best_weights=True)\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor],verbose=2,epochs=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.6265757027632013"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict(x_test)\n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC \n",
    "\n",
    "A receiver operating characteristic curve, or ROC curve, is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Senstivity and specificity \n",
    "\n",
    "senstivity = Tp/(Tp+Fn)\n",
    "\n",
    "specificty = Tn/(Tn+Fp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-244db471869c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\class\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[1;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[0;32m   2311\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'local_ns'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2312\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2313\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2314\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<C:\\Users\\THE HIRWANI\\Anaconda3\\envs\\class\\lib\\site-packages\\decorator.py:decorator-gen-109>\u001b[0m in \u001b[0;36mmatplotlib\u001b[1;34m(self, line)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\class\\lib\\site-packages\\IPython\\core\\magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\class\\lib\\site-packages\\IPython\\core\\magics\\pylab.py\u001b[0m in \u001b[0;36mmatplotlib\u001b[1;34m(self, line)\u001b[0m\n\u001b[0;32m     97\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Available matplotlib backends: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbackends_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m             \u001b[0mgui\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_matplotlib\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgui\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_show_matplotlib_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgui\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "%matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "mu1 = -2\n",
    "mu2 = 2\n",
    "variance = 1\n",
    "sigma = math.sqrt(variance)\n",
    "x1 = np.linspace(mu1 - 5*sigma, mu1 + 4*sigma, 100)\n",
    "x2 = np.linspace(mu2 - 5*sigma, mu2 + 4*sigma, 100)\n",
    "plt.plot(x1, stats.norm.pdf(x1, mu1, sigma)/1,color=\"green\")\n",
    "plt.plot(x2, stats.norm.pdf(x2, mu2, sigma)/1,color=\"red\")\n",
    "plt.axvline(x=-2,color=\"black\")\n",
    "plt.axvline(x=0,color=\"black\")\n",
    "plt.axvline(x=+2,color=\"black\")\n",
    "plt.text(-2.7,0.55,\"Sensative\")\n",
    "plt.text(-0.7,0.55,\"Balanced\")\n",
    "plt.text(1.7,0.55,\"Specific\")\n",
    "plt.ylim([0,0.53])\n",
    "plt.xlim([-5,5])\n",
    "plt.legend(['Negative','Positive'])\n",
    "plt.yticks([])\n",
    "#plt.set_yticklabels([])\n",
    "plt.show()\n",
    "\n",
    "def plot_roc(pred,y):\n",
    "    fpr, tpr, _ = roc_curve(y, pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC)')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>mean_radius</th>\n",
       "      <th>mean_texture</th>\n",
       "      <th>mean_perimeter</th>\n",
       "      <th>mean_area</th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>mean_compactness</th>\n",
       "      <th>mean_concavity</th>\n",
       "      <th>mean_concave_points</th>\n",
       "      <th>...</th>\n",
       "      <th>worst_radius</th>\n",
       "      <th>worst_texture</th>\n",
       "      <th>worst_perimeter</th>\n",
       "      <th>worst_area</th>\n",
       "      <th>worst_smoothness</th>\n",
       "      <th>worst_compactness</th>\n",
       "      <th>worst_concavity</th>\n",
       "      <th>worst_concave_points</th>\n",
       "      <th>worst_symmetry</th>\n",
       "      <th>worst_fractal_dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  mean_radius  mean_texture  mean_perimeter  mean_area  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   mean_smoothness  mean_compactness  mean_concavity  mean_concave_points  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  worst_radius  worst_texture  worst_perimeter  worst_area  \\\n",
       "0  ...         25.38          17.33           184.60      2019.0   \n",
       "1  ...         24.99          23.41           158.80      1956.0   \n",
       "2  ...         23.57          25.53           152.50      1709.0   \n",
       "3  ...         14.91          26.50            98.87       567.7   \n",
       "4  ...         22.54          16.67           152.20      1575.0   \n",
       "\n",
       "   worst_smoothness  worst_compactness  worst_concavity  worst_concave_points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   worst_symmetry  worst_fractal_dimension  \n",
       "0          0.4601                  0.11890  \n",
       "1          0.2750                  0.08902  \n",
       "2          0.3613                  0.08758  \n",
       "3          0.6638                  0.17300  \n",
       "4          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from scipy.stats import zscore\n",
    "\n",
    "path = './data/'\n",
    "\n",
    "\n",
    "read_csv = os.path.join(path, \"breast.csv\")\n",
    "df = pd.read_csv(read_csv , na_values = ['NA' , '?'])\n",
    "\n",
    "\n",
    "display(df[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>mean_radius</th>\n",
       "      <th>mean_texture</th>\n",
       "      <th>mean_perimeter</th>\n",
       "      <th>mean_area</th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>mean_compactness</th>\n",
       "      <th>mean_concavity</th>\n",
       "      <th>mean_concave_points</th>\n",
       "      <th>...</th>\n",
       "      <th>worst_radius</th>\n",
       "      <th>worst_texture</th>\n",
       "      <th>worst_perimeter</th>\n",
       "      <th>worst_area</th>\n",
       "      <th>worst_smoothness</th>\n",
       "      <th>worst_compactness</th>\n",
       "      <th>worst_concavity</th>\n",
       "      <th>worst_concave_points</th>\n",
       "      <th>worst_symmetry</th>\n",
       "      <th>worst_fractal_dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>1.097064</td>\n",
       "      <td>-2.073335</td>\n",
       "      <td>1.269934</td>\n",
       "      <td>0.984375</td>\n",
       "      <td>1.568466</td>\n",
       "      <td>3.283515</td>\n",
       "      <td>2.652874</td>\n",
       "      <td>2.532475</td>\n",
       "      <td>...</td>\n",
       "      <td>1.886690</td>\n",
       "      <td>-1.359293</td>\n",
       "      <td>2.303601</td>\n",
       "      <td>2.001237</td>\n",
       "      <td>1.307686</td>\n",
       "      <td>2.616665</td>\n",
       "      <td>2.109526</td>\n",
       "      <td>2.296076</td>\n",
       "      <td>2.750622</td>\n",
       "      <td>1.937015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>1.829821</td>\n",
       "      <td>-0.353632</td>\n",
       "      <td>1.685955</td>\n",
       "      <td>1.908708</td>\n",
       "      <td>-0.826962</td>\n",
       "      <td>-0.487072</td>\n",
       "      <td>-0.023846</td>\n",
       "      <td>0.548144</td>\n",
       "      <td>...</td>\n",
       "      <td>1.805927</td>\n",
       "      <td>-0.369203</td>\n",
       "      <td>1.535126</td>\n",
       "      <td>1.890489</td>\n",
       "      <td>-0.375612</td>\n",
       "      <td>-0.430444</td>\n",
       "      <td>-0.146749</td>\n",
       "      <td>1.087084</td>\n",
       "      <td>-0.243890</td>\n",
       "      <td>0.281190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>1.579888</td>\n",
       "      <td>0.456187</td>\n",
       "      <td>1.566503</td>\n",
       "      <td>1.558884</td>\n",
       "      <td>0.942210</td>\n",
       "      <td>1.052926</td>\n",
       "      <td>1.363478</td>\n",
       "      <td>2.037231</td>\n",
       "      <td>...</td>\n",
       "      <td>1.511870</td>\n",
       "      <td>-0.023974</td>\n",
       "      <td>1.347475</td>\n",
       "      <td>1.456285</td>\n",
       "      <td>0.527407</td>\n",
       "      <td>1.082932</td>\n",
       "      <td>0.854974</td>\n",
       "      <td>1.955000</td>\n",
       "      <td>1.152255</td>\n",
       "      <td>0.201391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>-0.768909</td>\n",
       "      <td>0.253732</td>\n",
       "      <td>-0.592687</td>\n",
       "      <td>-0.764464</td>\n",
       "      <td>3.283553</td>\n",
       "      <td>3.402909</td>\n",
       "      <td>1.915897</td>\n",
       "      <td>1.451707</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.281464</td>\n",
       "      <td>0.133984</td>\n",
       "      <td>-0.249939</td>\n",
       "      <td>-0.550021</td>\n",
       "      <td>3.394275</td>\n",
       "      <td>3.893397</td>\n",
       "      <td>1.989588</td>\n",
       "      <td>2.175786</td>\n",
       "      <td>6.046041</td>\n",
       "      <td>4.935010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>1.750297</td>\n",
       "      <td>-1.151816</td>\n",
       "      <td>1.776573</td>\n",
       "      <td>1.826229</td>\n",
       "      <td>0.280372</td>\n",
       "      <td>0.539340</td>\n",
       "      <td>1.371011</td>\n",
       "      <td>1.428493</td>\n",
       "      <td>...</td>\n",
       "      <td>1.298575</td>\n",
       "      <td>-1.466770</td>\n",
       "      <td>1.338539</td>\n",
       "      <td>1.220724</td>\n",
       "      <td>0.220556</td>\n",
       "      <td>-0.313395</td>\n",
       "      <td>0.613179</td>\n",
       "      <td>0.729259</td>\n",
       "      <td>-0.868353</td>\n",
       "      <td>-0.397100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  mean_radius  mean_texture  mean_perimeter  mean_area  \\\n",
       "0    842302         M     1.097064     -2.073335        1.269934   0.984375   \n",
       "1    842517         M     1.829821     -0.353632        1.685955   1.908708   \n",
       "2  84300903         M     1.579888      0.456187        1.566503   1.558884   \n",
       "3  84348301         M    -0.768909      0.253732       -0.592687  -0.764464   \n",
       "4  84358402         M     1.750297     -1.151816        1.776573   1.826229   \n",
       "\n",
       "   mean_smoothness  mean_compactness  mean_concavity  mean_concave_points  \\\n",
       "0         1.568466          3.283515        2.652874             2.532475   \n",
       "1        -0.826962         -0.487072       -0.023846             0.548144   \n",
       "2         0.942210          1.052926        1.363478             2.037231   \n",
       "3         3.283553          3.402909        1.915897             1.451707   \n",
       "4         0.280372          0.539340        1.371011             1.428493   \n",
       "\n",
       "   ...  worst_radius  worst_texture  worst_perimeter  worst_area  \\\n",
       "0  ...      1.886690      -1.359293         2.303601    2.001237   \n",
       "1  ...      1.805927      -0.369203         1.535126    1.890489   \n",
       "2  ...      1.511870      -0.023974         1.347475    1.456285   \n",
       "3  ...     -0.281464       0.133984        -0.249939   -0.550021   \n",
       "4  ...      1.298575      -1.466770         1.338539    1.220724   \n",
       "\n",
       "   worst_smoothness  worst_compactness  worst_concavity  worst_concave_points  \\\n",
       "0          1.307686           2.616665         2.109526              2.296076   \n",
       "1         -0.375612          -0.430444        -0.146749              1.087084   \n",
       "2          0.527407           1.082932         0.854974              1.955000   \n",
       "3          3.394275           3.893397         1.989588              2.175786   \n",
       "4          0.220556          -0.313395         0.613179              0.729259   \n",
       "\n",
       "   worst_symmetry  worst_fractal_dimension  \n",
       "0        2.750622                 1.937015  \n",
       "1       -0.243890                 0.281190  \n",
       "2        1.152255                 0.201391  \n",
       "3        6.046041                 4.935010  \n",
       "4       -0.868353                -0.397100  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_columns = df.columns.drop('diagnosis').drop('id')\n",
    "for col in x_columns:\n",
    "    df[col] = zscore(df[col])\n",
    "    \n",
    "x = df[x_columns].values\n",
    "    \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.09706398, -2.07333501,  1.26993369, ...,  2.29607613,\n",
       "         2.75062224,  1.93701461],\n",
       "       [ 1.82982061, -0.35363241,  1.68595471, ...,  1.0870843 ,\n",
       "        -0.24388967,  0.28118999],\n",
       "       [ 1.57988811,  0.45618695,  1.56650313, ...,  1.95500035,\n",
       "         1.152255  ,  0.20139121],\n",
       "       ...,\n",
       "       [ 0.70228425,  2.0455738 ,  0.67267578, ...,  0.41406869,\n",
       "        -1.10454895, -0.31840916],\n",
       "       [ 1.83834103,  2.33645719,  1.98252415, ...,  2.28998549,\n",
       "         1.91908301,  2.21963528],\n",
       "       [-1.80840125,  1.22179204, -1.81438851, ..., -1.74506282,\n",
       "        -0.04813821, -0.75120669]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['diagnosis'].map({'M':1,\"B\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-ee98eed1674e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m model.add(Dense(64,\n\u001b[0m\u001b[0;32m      2\u001b[0m                 \u001b[0mkernel_initializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'random_uniform'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                 bias_initializer='zeros'))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.add(Dense(64,\n",
    "                kernel_initializer='random_uniform',\n",
    "                bias_initializer='zeros'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 426 samples, validate on 143 samples\n",
      "Epoch 1/1000\n",
      " - 1s - loss: 1.6500 - acc: 0.6291 - val_loss: 1.2174 - val_acc: 0.6224\n",
      "Epoch 2/1000\n",
      " - 0s - loss: 1.0165 - acc: 0.6291 - val_loss: 0.7569 - val_acc: 0.6224\n",
      "Epoch 3/1000\n",
      " - 0s - loss: 0.6060 - acc: 0.6643 - val_loss: 0.3781 - val_acc: 0.8182\n",
      "Epoch 4/1000\n",
      " - 0s - loss: 0.3304 - acc: 0.8427 - val_loss: 0.2577 - val_acc: 0.9161\n",
      "Epoch 5/1000\n",
      " - 0s - loss: 0.1854 - acc: 0.9319 - val_loss: 0.2024 - val_acc: 0.9580\n",
      "Epoch 6/1000\n",
      " - 0s - loss: 0.1711 - acc: 0.9624 - val_loss: 0.1809 - val_acc: 0.9650\n",
      "Epoch 7/1000\n",
      " - 0s - loss: 0.0916 - acc: 0.9695 - val_loss: 0.0868 - val_acc: 0.9650\n",
      "Epoch 8/1000\n",
      " - 0s - loss: 0.0758 - acc: 0.9742 - val_loss: 0.0741 - val_acc: 0.9720\n",
      "Epoch 9/1000\n",
      " - 0s - loss: 0.0642 - acc: 0.9789 - val_loss: 0.0627 - val_acc: 0.9790\n",
      "Epoch 10/1000\n",
      " - 0s - loss: 0.0576 - acc: 0.9812 - val_loss: 0.0574 - val_acc: 0.9790\n",
      "Epoch 11/1000\n",
      " - 0s - loss: 0.0512 - acc: 0.9812 - val_loss: 0.0569 - val_acc: 0.9720\n",
      "Epoch 12/1000\n",
      " - 0s - loss: 0.0461 - acc: 0.9836 - val_loss: 0.0536 - val_acc: 0.9860\n",
      "Epoch 13/1000\n",
      " - 0s - loss: 0.0426 - acc: 0.9859 - val_loss: 0.0543 - val_acc: 0.9790\n",
      "Epoch 14/1000\n",
      " - 0s - loss: 0.0406 - acc: 0.9859 - val_loss: 0.0526 - val_acc: 0.9860\n",
      "Epoch 15/1000\n",
      " - 0s - loss: 0.0375 - acc: 0.9883 - val_loss: 0.0543 - val_acc: 0.9860\n",
      "Epoch 16/1000\n",
      " - 0s - loss: 0.0345 - acc: 0.9883 - val_loss: 0.0517 - val_acc: 0.9860\n",
      "Epoch 17/1000\n",
      " - 0s - loss: 0.0329 - acc: 0.9883 - val_loss: 0.0507 - val_acc: 0.9860\n",
      "Epoch 18/1000\n",
      " - 0s - loss: 0.0301 - acc: 0.9930 - val_loss: 0.0535 - val_acc: 0.9860\n",
      "Epoch 19/1000\n",
      " - 0s - loss: 0.0283 - acc: 0.9930 - val_loss: 0.0554 - val_acc: 0.9860\n",
      "Epoch 20/1000\n",
      " - 0s - loss: 0.0261 - acc: 0.9930 - val_loss: 0.0588 - val_acc: 0.9790\n",
      "Epoch 21/1000\n",
      " - 0s - loss: 0.0250 - acc: 0.9930 - val_loss: 0.0607 - val_acc: 0.9790\n",
      "Epoch 00021: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x23a1b78c908>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim=x.shape[1], activation='relu',kernel_initializer='random_normal'))\n",
    "model.add(Dense(50,activation='relu',kernel_initializer='random_normal'))\n",
    "model.add(Dense(25,activation='relu',kernel_initializer='random_normal'))\n",
    "model.add(Dense(1,activation='linear',kernel_initializer='random_normal'))\n",
    "model.compile(loss='binary_crossentropy',optimizer=tensorflow.keras.optimizers.Adam(),metrics =['accuracy'])\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3,patience=5, verbose=1, mode='auto')\n",
    "model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor],verbose=2,epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "      \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "def plot_roc(pred,y):\n",
    "    fpr, tpr, _ = roc_curve(y, pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC)')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = model.predict(x_test)\n",
    "plot_roc(pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
